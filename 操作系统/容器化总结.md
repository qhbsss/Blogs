[TOC]
# Linux版本
## Linux介绍
Linux，最早由Linus Benedict Torvalds在1991年开始编写。在这之前，Richard Stallman创建了Free Software Foundation（FSF）组织以及GNU项目，并不断的编写创建GNU程序（此类程序的许可方式均为GPL: General Public License）。在不断的有杰出的程序员和开发者加入到GNU组织中后，便造就了今天我们所看到的Linux，或称GNU/Linux。
## Linux 发行版
Linux的发行版本可以大体分为两类，一类是商业公司维护的发行版本，一类是社区组织维护的发行版本
前者以著名的Redhat（RHEL）为代表，后者以Debian为代表。
### Redhat
Redhat，应该称为Redhat系列，包括RHEL(Redhat Enterprise Linux，也就是所谓的Redhat Advance Server，收费版本)、Fedora Core(由原来的Redhat桌面版本发展而来，免费版本)、CentOS(RHEL的社区克隆版本，免费)。Redhat应该说是在国内使用人群最多的Linux版本，甚至有人将Redhat等同于Linux，而有些老鸟更是只用这一个版本的Linux。所以这个版本的特点就是使用人群数量大，资料非常多，言下之意就是如果你有什么不明白的地方，很容易找到人来问，而且网上的一般Linux教程都是以Redhat为例来讲解的。**Redhat系列的包管理方式采用的是基于RPM包的YUM包管理方式，包分发方式是编译好的二进制文件。**稳定性方面RHEL和CentOS的稳定性非常好，适合于服务器使用，但是Fedora Core的稳定性较差，最好只用于桌面应用。
### Debian
Debian，或者称Debian系列，包括Debian和Ubuntu等。Debian是社区类Linux的典范，是迄今为止最遵循GNU规范的Linux系统。Debian最早由Ian Murdock于1993年创建，分为三个版本分支（branch）： stable, testing 和 unstable。其中，unstable为最新的测试版本，其中包括最新的软件包，但是也有相对较多的bug，适合桌面用户。testing的版本都经 过unstable中的测试，相对较为稳定，也支持了不少新技术（比如SMP等）。而stable一般只用于服务器，上面的软件包大部分都比较过时，但是 稳定和安全性都非常的高。**Debian最具特色的是apt-get / dpkg包管理方式**，其实Redhat的YUM也是在模仿Debian的APT方式，但在二进制文件发行方式中，APT应该是最好的了。Debian的资 料也很丰富，有很多支持的社区，有问题求教也有地方可去:)
>Ubuntu，严格来说不能算一个独立的发行版本，Ubuntu是基于Debian的unstable版本加强而来，可以这么说，Ubuntu就是 一个拥有Debian所有的优点，以及自己所加强的优点的近乎完美的 Linux桌面系统。根据选择的桌面系统不同，有三个版本可供选择，基于Gnome的Ubuntu，基于KDE的Kubuntu以及基于Xfc的 Xubuntu。特点是界面非常友好，容易上手，对硬件的支持非常全面，是最适合做桌面系统的Linux发行版本。

# 容器目录与宿主机目录的关系
>参考：https://www.cnblogs.com/ygbh/p/17248397.html
## 内部存储
Docker的文件系统 与Docker容器具有相同的生命周期，但是Docker容器肯定会遇到同时运行到多节
点场景中，这个时候，会因为节点崩溃、服务崩溃、网络原因，导致容器异常退出，所以一旦我们将数据存储到容器内部，肯定会导致数据丢失。
Docker镜像是只读的文件，Docker容器可读可写，但是不能够数据持久化。所以为了避免这种数据异常丢失的情况，我们一般会将 容器的数据(程序状态数据、应用存储数据等)，存储在容器的文件系统之外。
## 外部存储
![image1](https://img2023.cnblogs.com/blog/918540/202303/918540-20230323180056858-193319136.png)
通过数据卷 或者 数据卷容器，将当前宿主机上面的文件系统目录与容器里面的工作目录进行一一的关联。
![image2](https://img2023.cnblogs.com/blog/918540/202303/918540-20230323191444368-351251437.png)
### k8s的存储类型
| 类型  | 举例  |
| ------------ | ------------ |
|  本地数据卷 | emptyDir、hostPath、local等  |
| 云存储数据卷  | gcePersistentDisk、awsElasticBlockStore等  |
| 网络存储卷  | NFS、gitRepo、NAS、SAN等  |
| 分布式存储卷  | GlusterFS、CephFS、rdb(块设备)、iscsi等  |
| 信息数据卷  |  flocker、secret等 |
|   | k8s还支持 CSI(Container Storage Interface)接口方式，来实现更大范围的存储功能扩展。  |
### 资源对象属性介绍
```yaml
spec:
  volumes:
  - name <string>      # 存储卷名称标识，仅可使用DNS标签格式的字符，在当前Pod中必须唯一（卷的名字）
    VOL_TYPE <Object>  # 存储卷插件及具体的目标存储供给方的相关配置（卷的类型）
    hostPath <String>  # 宿主机上挂载卷的路径（如果是hostPath方式存储的话）
  containers:
  - name: …
    image: …
    volumeMounts:
    - name <string>        # 要挂载的存储卷的名称，必须匹配存储卷列表中某项的定义（上面卷的名字）
      mountPath <string>   # 容器文件系统上的挂载点路径（容器上挂载卷的路径）
      readOnly <boolean>   # 是否挂载为只读模式，默认为“否”
      subPath <string>     # 挂载存储卷上的一个子目录至指定的挂载点
      subPathExpr <string> # 挂载由指定的模式匹配到的存储卷的文件或目录至挂载点
```
### emptyDir
一个emptyDir volume在pod被调度到某个Node时候自动创建的，无需指定宿主机上对应的目录。特点如下：
1、跟随 Pod 初始化而来，开始是空数据卷
2、Pod 移除，emptyDir中数据随之永久消除
3、emptyDir 一般做本地临时存储空间使用
4、emptyDir 数据卷介质种类跟当前主机的磁盘一样。
```yaml
# 定义资源配置清单
cat >storage-empty.yml<<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: nginx-volume
  labels:
    name: nginx-volume
spec:
  volumes:
  - name: empty-volume
    emptyDir: {}
  containers:
    - name: nginx-volume
      image: 192.168.10.33:80/k8s/my_nginx:v1
      volumeMounts:
       - name: empty-volume
         mountPath: /nginx/www/empty/
EOF

# 应用资源配置清单 
master1 storage]# kubectl apply  -f storage-empty.yml 

master1 storage]# kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
nginx-volume   1/1     Running   0          69s

# 查看挂载的信息
master1 storage]# kubectl describe pod nginx-volume 
...
    Mounts:
      /nginx/www/empty/ from empty-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kd8dw (ro)
...

# 查看目录是否存在
master1 storage]# kubectl exec nginx-volume -- ls /nginx/www
empty

master1 storage]# kubectl exec -it nginx-volume -- /bin/bash
root@nginx-volume:/#

# 往文件目录写入数据
master1 storage]# kubectl exec -it nginx-volume -- /bin/bash
root@nginx-volume:/# echo "nihao" > /nginx/www/nihao.txt

# 到节点查询文件位置
node1 ~]# find / -name "nihao.txt"
/run/containerd/io.containerd.runtime.v2.task/k8s.io/ec0ce61aa680a89da7d97ea631332e131b05840913e4d691a984516bf35a3e0f/rootfs/nginx/www/nihao.txt
/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/2060/fs/nginx/www/nihao.txt
```
### hostPath
hostPath将宿主机上的目录挂载到Pod中作为数据的存储目录，一般用在如下场景：
1、容器应用程序中某些文件需要永久保存
2、某些容器应用需要用到容器的内部数据结构，可将宿主机的/var/lib/docker挂载到Pod中
3、Pod删除，宿主机文件不受影响
hostPath使用注意事项：
1、不同宿主机的文件内容不一定完全相同，所以Pod迁移前后的访问效果不一样
2、宿主机的目录不属于资源对象的资源，所以我们对资源设置的资源配额限制对hostPath目录无效    
```yaml
# 定义资源配置清单
cat >hostpath-redis.yml<<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-redis
spec:
  nodeName: node2
  volumes:
  - name: redis-backup
    hostPath:
     path: /backup/redis
  containers:
    - name: hostpath-redis
      image: 192.168.10.33:80/k8s/redis:latest
      volumeMounts:
       - name: redis-backup
         mountPath: /data
EOF

# 关键点：
#     spec.containers.volumeMounts的name属性和spec.volumes的那么属性完全一致,因为他们是基于name来关联的。  
# 注意：
#     redis的镜像将数据保存到了容器的/data目录下。

# 应用资源配置清单并且查询目录的挂载情况
master1 ]# kubectl apply -f hostpath-redis.yml 
pod/hostpath-redis created

master1 ]# kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES
hostpath-redis   1/1     Running   0          43s   10.244.4.100   node2   <none>           <none>

# 本地目录存在
node2 ~]# ll /backup/redis/
total 0


# 查看pod的描述信息
master1 ]# kubectl describe pod hostpath-redis
...
    Environment:  <none>
    Mounts:
      /data from redis-backup (rw)
Volumes:
  redis-backup:
    Type:          HostPath (bare host directory volume)
    Path:          /backup/redis
...
```
# 容器的底层实现原理
容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。
对于Docker等大多数Linux容器来说，Cgroups技术是用来制造约束的主要手段，而Namespace技术则是用来修改进程视图的主要方法。
## NameSpace
Namespace的使用方式：它其实是Linux创建新进程的一个可选参数。在Linux系统中创建进程的系统调用是clone()，比如：
```c
int pid = clone(main_function, stack_size, SIGCHLD, NULL); 
```
这个系统调用就会为我们创建一个新的进程，并且返回它的进程号pid。

而当我们用clone()系统调用创建一个新进程时，就可以在参数中指定CLONE_NEWPID参数，比如：
```c
int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 
```
这时，新创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的PID是1。之所以说“看到”，是因为这只是一个“障眼法”，在宿主机真实的进程空间里，这个进程的PID还是真实的数值，比如100。
### eg:
1. 首先创建一个容器
```bash
docker run -it busybox /bin/sh
```
这个命令是Docker项目最重要的一个操作，即大名鼎鼎的docker run。

而-it参数告诉了Docker项目在启动容器后，需要给我们分配一个文本输入/输出环境，也就是TTY，跟容器的标准输入相关联，这样我们就可以和这个Docker容器进行交互了。而/bin/sh就是我们要在Docker容器里运行的程序。
2. 在容器里执行一下ps指令
```bash
PID  USER   TIME COMMAND
  1 root   0:00 /bin/sh
  10 root   0:00 ps
```
在Docker里最开始执行的/bin/sh，就是这个容器内部的第1号进程（PID=1），而这个容器里一共只有两个进程在运行。这就意味着，前面执行的/bin/sh，以及我们刚刚执行的ps，已经被Docker隔离在了一个跟宿主机完全不同的世界当中。
在宿主机上运行了一个/bin/sh程序，操作系统都会给它分配一个进程编号，比如PID=100。这个编号是进程的唯一标识。通过Docker把这个/bin/sh程序运行在一个容器当中。这时候，Docker就会施一个“障眼法”，让他永远看不到前面的其他99个进程。
这种机制，其实就是对被隔离应用的进程空间做了手脚，使得这些进程只能看到重新计算过的进程编号，比如PID=1。可实际上，他们在宿主机的操作系统里，还是原来的第100号进程。

## Cgroups
Linux Cgroups的全称是Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括CPU、内存、磁盘、网络带宽等等。
在Linux中，Cgroups给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的/sys/fs/cgroup路径下。可以用mount指令把它们展示出来，这条命令是：    
```bash
mount -t cgroup 
cpuset on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cpu on /sys/fs/cgroup/cpu type cgroup (rw,nosuid,nodev,noexec,relatime,cpu)
cpuacct on /sys/fs/cgroup/cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct)
blkio on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
memory on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)

```
在/sys/fs/cgroup下面有很多诸如cpuset、cpu、 memory这样的子目录，也叫子系统。这些都是我这台机器当前可以被Cgroups进行限制的资源种类。而在子系统对应的资源种类下，你就可以看到该类资源具体可以被限制的方法。比如，对CPU子系统来说，我们就可以看到如下几个配置文件，这个指令是：
```bash
ls /sys/fs/cgroup/cpu
cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release
cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks
```
输出里注意到cfs_period和cfs_quota这样的关键词。这两个参数需要组合使用，可以用来限制进程在长度为cfs_period的一段时间内，只能被分配到总量为cfs_quota的CPU时间。
除CPU子系统外，Cgroups的每一个子系统都有其独有的资源限制能力，比如：
blkio，为​​​块​​​设​​​备​​​设​​​定​​​I/O限​​​制，一般用于磁盘等设备；
cpuset，为进程分配单独的CPU核和对应的内存节点；
memory，为进程设定内存使用的限制。
Linux Cgroups的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。而对于Docker等Linux容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的PID填写到对应控制组的tasks文件中就可以了。
### eg:
在对应的子系统下面创建一个目录，比如，我们现在进入/sys/fs/cgroup/cpu目录下：
```bash
root@ubuntu:/sys/fs/cgroup/cpu$ mkdir container
root@ubuntu:/sys/fs/cgroup/cpu$ ls container/
cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release
cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks
```
这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的container目录下，自动生成该子系统对应的资源限制文件。

现在，我们在后台执行这样一条脚本：
```bash
$ while : ; do : ; done &
[1] 226
```
显然，它执行了一个死循环，可以把计算机的CPU吃到100%，根据它的输出，我们可以看到这个脚本在后台运行的进程号（PID）是226。

这样，我们可以用top指令来确认一下CPU有没有被打满：
```bash
$ top
%Cpu0 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st
```
在输出里可以看到，CPU的使用率已经100%了（%Cpu0 :100.0 us）。

而此时，我们可以通过查看container目录下的文件，看到container控制组里的CPU quota还没有任何限制（即：-1），CPU period则是默认的100 ms（100000 us）：
```bash
$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 
-1
$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us 
100000
```
接下来，我们可以通过修改这些文件的内容来设置限制。

比如，向container组里的cfs_quota文件写入20 ms（20000 us）：
```bash
$ echo 20000 > /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us
```
它意味着在每100 ms的时间里，被该控制组限制的进程只能使用20 ms的CPU时间，也就是说这个进程只能使用到20%的CPU带宽。

接下来，我们把被限制的进程的PID写入container组里的tasks文件，上面的设置就会对该进程生效了：
```bash
echo 226 > /sys/fs/cgroup/cpu/container/tasks 
```
用top指令查看一下,可以看到，计算机的CPU使用率立刻降到了20%（%Cpu0 : 20.3 us）。
```bash
top
%Cpu0 : 20.3 us, 0.0 sy, 0.0 ni, 79.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st
```
Docker等Linux容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的PID填写到对应控制组的tasks文件中就可以了。

而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行docker run时的参数指定了，比如这样一条命令：
```bash
docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash
```
在启动这个容器后，我们可以通过查看Cgroups文件系统下，CPU子系统中，“docker”这个控制组里的资源限制文件的内容来确认：
```bash
$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us 
100000
$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us 
20000

```
这就意味着这个Docker容器，只能使用到20%的CPU带宽。
## 容器镜像
### Mount Namespace
Mount Namespace：容器里的应用进程，理应看到一份完全独立的文件系统。这样，它就可以在自己的容器目录（比如/tmp）下进行操作，而完全不会受宿主机以及其他容器的影响。
**Mount Namespace跟其他Namespace的使用略有不同的地方：它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。**
### eg:
1. 创建进程之前不挂载目录
```c
#define _GNU_SOURCE
#include <sys/mount.h> 
#include <sys/types.h>
#include <sys/wait.h>
#include <stdio.h>
#include <sched.h>
#include <signal.h>
#include <unistd.h>
#define STACK_SIZE (1024 * 1024)
static char container_stack[STACK_SIZE];
char* const container_args[] = {
  "/bin/bash",
  NULL
};

int container_main(void* arg)
{  
  printf("Container - inside the container!\n");
  execv(container_args[0], container_args);
  printf("Something's wrong!\n");
  return 1;
}

int main()
{
  printf("Parent - start a container!\n");
  int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWNS | SIGCHLD , NULL);
  waitpid(container_pid, NULL, 0);
  printf("Parent - container stopped!\n");
  return 0;
}
```
这段代码的功能非常简单：在main函数里，我们通过clone()系统调用创建了一个新的子进程container_main，并且声明要为它启用Mount Namespace（即：CLONE_NEWNS标志）。

而这个子进程执行的，是一个“/bin/bash”程序，也就是一个shell。所以这个shell就运行在了Mount Namespace的隔离环境中。

我们来一起编译一下这个程序：
```bash
$ gcc -o ns ns.c
$ ./ns
Parent - start a container!
Container - inside the container!
```
这样，我们就进入了这个“容器”当中。可是，如果在“容器”里执行一下ls指令的话，我们就会发现一个有趣的现象： /tmp目录下的内容跟宿主机的内容是一样的。
```bash
$ ls /tmp
# 你会看到好多宿主机的文件
```
即使开启了Mount Namespace，容器进程看到的文件系统也跟宿主机完全一样。
2. 创建进程前挂载目录
创建新进程时，除了声明要启用Mount Namespace之外，我们还可以告诉容器进程，有哪些目录需要重新挂载，就比如这个/tmp目录。于是，我们在容器进程执行前可以添加一步重新挂载 /tmp目录的操作：
```c
int container_main(void* arg)
{
  printf("Container - inside the container!\n");
  // 如果你的机器的根目录的挂载类型是shared，那必须先重新挂载根目录
  // mount("", "/", NULL, MS_PRIVATE, "");
  mount("none", "/tmp", "tmpfs", 0, "");
  execv(container_args[0], container_args);
  printf("Something's wrong!\n");
  return 1;
}
```
在修改后的代码里，我在容器进程启动之前，加上了一句mount(“none”, “/tmp”, “tmpfs”, 0, “”)语句。就这样，我告诉了容器以tmpfs（内存盘）格式，重新挂载了/tmp目录。

编译执行后试验一下：
```bash
$ gcc -o ns ns.c
$ ./ns
Parent - start a container!
Container - inside the container!
$ ls /tmp
```

可以看到，这次/tmp变成了一个空目录，这意味着重新挂载生效了。我们可以用mount -l检查一下：
```bash
$ mount -l | grep tmpfs
none on /tmp type tmpfs (rw,relatime)
```
可以看到，容器里的/tmp目录是以tmpfs方式单独挂载的。
更重要的是，因为我们创建的新进程启用了Mount Namespace，所以这次重新挂载的操作，只在容器进程的Mount Namespace中有效。如果在宿主机上用mount -l来检查一下这个挂载，你会发现它是不存在的：
```bash
# 在宿主机上
$ mount -l | grep tmpfs
```
### chroot
可以在容器进程启动之前重新挂载它的整个根目录“/”。而由于Mount Namespace的存在，这个挂载对宿主机不可见，所以容器进程就可以在里面随便折腾了。

在Linux操作系统里，有一个名为chroot的命令可以帮助你在shell中方便地完成这个工作。

实际上，Mount Namespace正是基于对chroot的不断改良才被发明出来的，它也是Linux操作系统里的第一个Namespace。

当然，为了能够让容器的这个根目录看起来更“真实”，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如Ubuntu16.04的ISO。这样，在容器启动之后，我们在容器里通过执行"ls /"查看根目录下的内容，就是Ubuntu 16.04的所有目录和文件。

而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。
需要明确的是，**rootfs只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在Linux操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像**。
对一个应用来说，操作系统本身才是它运行所需要的最完整的“依赖库”。由于rootfs里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。
## 共享文件系统
>问题：难道我每开发一个应用，或者升级一下现有的应用，都要重复制作一次rootfs吗？
Docker在镜像的设计中，引入了层（layer）的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量rootfs。用到了一种叫作联合文件系统（Union File System）的能力。Union File System也叫UnionFS，最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下。
### eg:
比如，我现在有两个目录A和B，它们分别有两个文件：
```bash
$ tree
.
├── A
│  ├── a
│  └── x
└── B
  ├── b
  └── x
```
然后，我使用联合挂载的方式，将这两个目录挂载到一个公共的目录C上：
```bash
$ mkdir C
$ mount -t aufs -o dirs=./A:./B none ./C
```

这时，我再查看目录C的内容，就能看到目录A和B下的文件被合并到了一起：


```
$ tree ./C
./C
├── a
├── b
└── x
```
可以看到，在这个合并后的目录C里，有a、b、x三个文件，并且x文件只有一份。这，就是“合并”的含义。此外，如果你在目录C里对a、b、x文件做修改，这些修改也会在对应的目录A、B中生效。

AuFS的全称是Another UnionFS,对于AuFS来说，它最关键的目录结构在/var/lib/docker路径下的diff目录：

```
/var/lib/docker/aufs/diff/<layer_id>
```
### eg:
现在，我们启动一个容器，比如：


```
$ docker run -d ubuntu:latest sleep 3600
```
这时候，Docker就会从Docker Hub上拉取一个Ubuntu镜像到本地。

这个所谓的“镜像”，实际上就是一个Ubuntu操作系统的rootfs，它的内容是Ubuntu操作系统的所有文件和目录。不过，与之前我们讲述的rootfs稍微不同的是，Docker镜像使用的rootfs，往往由多个“层”组成：


```
$ docker image inspect ubuntu:latest
...
     "RootFS": {
      "Type": "layers",
      "Layers": [
        "sha256:f49017d4d5ce9c0f544c...",
        "sha256:8f2b771487e9d6354080...",
        "sha256:ccd4d61916aaa2159429...",
        "sha256:c01d74f99de40e097c73...",
        "sha256:268a067217b5fe78e000..."
      ]
    }
```
可以看到，这个Ubuntu镜像，实际上由五个层组成。这五个层就是五个增量rootfs，每一层都是Ubuntu操作系统文件与目录的一部分；而在使用镜像时，Docker会把这些增量联合挂载在一个统一的挂载点上（等价于前面例子里的“/C”目录）。

这个挂载点就是/var/lib/docker/aufs/mnt/，比如：


```
/var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3e
```
不出意外的，这个目录里面正是一个完整的Ubuntu操作系统：


```
$ ls /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3e
bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var
```
那么，前面提到的五个镜像层，又是如何被联合挂载成这样一个完整的Ubuntu文件系统的呢？

这个信息记录在AuFS的系统目录/sys/fs/aufs下面。

首先，通过查看AuFS的挂载信息，我们可以找到这个目录对应的AuFS的内部ID（也叫：si）：


```
$ cat /proc/mounts| grep aufs
none /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fc... aufs rw,relatime,si=972c6d361e6b32ba,dio,dirperm1 0 0
即，si=972c6d361e6b32ba。
```

然后使用这个ID，你就可以在/sys/fs/aufs下查看被联合挂载在一起的各个层的信息：

```
$ cat /sys/fs/aufs/si_972c6d361e6b32ba/br[0-9]*
/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc...=rw
/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc...-init=ro+wh
/var/lib/docker/aufs/diff/32e8e20064858c0f2...=ro+wh
/var/lib/docker/aufs/diff/2b8858809bce62e62...=ro+wh
/var/lib/docker/aufs/diff/20707dce8efc0d267...=ro+wh
/var/lib/docker/aufs/diff/72b0744e06247c7d0...=ro+wh
/var/lib/docker/aufs/diff/a524a729adadedb90...=ro+wh
```
从这些信息里，我们可以看到，镜像的层都放置在/var/lib/docker/aufs/diff目录下，然后被联合挂载在/var/lib/docker/aufs/mnt里面。

![](https://ilearning.huawei.com/edx/catalog/api/v1/third_partner/proxy/static001.geekbang.org/resource/image/8a/5f/8a7b5cfabaab2d877a1d4566961edd5f.png?wh=1350*1002)

# 容器执行的原理
## 制作镜像
### eg:
用Docker部署一个用Python编写的Web应用。这个应用的代码部分（app.py）非常简单：
```bash
from flask import Flask
import socket
import os

app = Flask(__name__)

@app.route('/')
def hello():
    html = "<h3>Hello {name}!</h3>" \
           "<b>Hostname:</b> {hostname}<br/>"           
    return html.format(name=os.getenv("NAME", "world"), hostname=socket.gethostname())
    
if __name__ == "__main__":
    app.run(host='0.0.0.0', port=80)
```
在这段代码中，我使用Flask框架启动了一个Web服务器，而它唯一的功能是：如果当前环境中有“NAME”这个环境变量，就把它打印在“Hello”之后，否则就打印“Hello world”，最后再打印出当前环境的hostname。

这个应用的依赖，则被定义在了同目录下的requirements.txt文件里，内容如下所示：


```
$ cat requirements.txt
Flask
```
而将这样一个应用容器化的第一步，是制作容器镜像。

不过，相较于我之前介绍的制作rootfs的过程，Docker为你提供了一种更便捷的方式，叫作Dockerfile，如下所示。

```
# 使用官方提供的Python开发镜像作为基础镜像
FROM python:2.7-slim

# 将工作目录切换为/app
WORKDIR /app

# 将当前目录下的所有内容复制到/app下
ADD . /app

# 使用pip命令安装这个应用所需要的依赖
RUN pip install --trusted-host pypi.python.org -r requirements.txt

# 允许外界访问容器的80端口
EXPOSE 80

# 设置环境变量
ENV NAME World

# 设置容器进程为：python app.py，即：这个Python应用的启动命令
CMD ["python", "app.py"]
```
通过这个文件的内容，你可以看到Dockerfile的设计思想，是使用一些标准的原语（即大写高亮的词语），描述我们所要构建的Docker镜像。并且这些原语，都是按顺序处理的。

比如FROM原语，指定了“python:2.7-slim”这个官方维护的基础镜像，从而免去了安装Python等语言环境的操作。否则，这一段我们就得这么写了：


```
FROM ubuntu:latest
RUN apt-get update -yRUN apt-get install -y python-pip python-dev build-essential
...
```
其中，RUN原语就是在容器里执行shell命令的意思。

而WORKDIR，意思是在这一句之后，Dockerfile后面的操作都以这一句指定的/app目录作为当前目录。

所以，到了最后的CMD，意思是Dockerfile指定python app.py为这个容器的进程。这里，app.py的实际路径是/app/app.py。所以，CMD ["python", "app.py"]等价于"docker run <image> python app.py"。

另外，在使用Dockerfile时，你可能还会看到一个叫作ENTRYPOINT的原语。实际上，它和CMD都是Docker容器进程启动所必需的参数，完整执行格式是：“ENTRYPOINT CMD”。

但是，默认情况下，Docker会为你提供一个隐含的ENTRYPOINT，即：/bin/sh -c。所以，在不指定ENTRYPOINT时，比如在我们这个例子里，实际上运行在容器里的完整进程是：/bin/sh -c "python app.py"，即CMD的内容就是ENTRYPOINT的参数。

备注：基于以上原因，我们后面会统一称Docker容器的启动进程为ENTRYPOINT，而不是CMD。

需要注意的是，Dockerfile里的原语并不都是指对容器内部的操作。就比如ADD，它指的是把当前目录（即Dockerfile所在的目录）里的文件，复制到指定容器内的目录当中。

读懂这个Dockerfile之后，我再把上述内容，保存到当前目录里一个名叫“Dockerfile”的文件中：


```
$ ls
Dockerfile  app.py   requirements.txt
```
接下来，我就可以让Docker制作这个镜像了，在当前目录执行：


```
$ docker build -t helloworld .
```
其中，-t的作用是给这个镜像加一个Tag，即：起一个好听的名字。docker build会自动加载当前目录下的Dockerfile文件，然后按照顺序，执行文件中的原语。而这个过程，实际上可以等同于Docker使用基础镜像启动了一个容器，然后在容器中依次执行Dockerfile中的原语。

需要注意的是，**Dockerfile中的每个原语执行后，都会生成一个对应的镜像层**。即使原语本身并没有明显地修改文件的操作（比如，ENV原语），它对应的层也会存在。只不过在外界看来，这个层是空的。

docker build操作完成后，我可以通过docker images命令查看结果：


```
$ docker image ls

REPOSITORY            TAG                 IMAGE ID
helloworld         latest              653287cdf998
```
通过这个镜像ID，你就可以查看这些新增的层在AuFS路径下对应的文件和目录了。

接下来，我使用这个镜像，通过docker run命令启动容器：


```
$ docker run -p 4000:80 helloworld
```
在这一句命令中，镜像名helloworld后面，我什么都不用写，因为在Dockerfile中已经指定了CMD。否则，我就得把进程的启动命令加在后面：


```
$ docker run -p 4000:80 helloworld python app.py
```
容器启动之后，我可以使用docker ps命令看到：


```
$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED
4ddf4638572d        helloworld       "python app.py"     10 seconds ago
```
同时，我已经通过-p 4000:80告诉了Docker，请把容器内的80端口映射在宿主机的4000端口上。

这样做的目的是，只要访问宿主机的4000端口，我就可以看到容器里应用返回的结果：


```
$ curl http://localhost:4000
<h3>Hello World!</h3><b>Hostname:</b> 4ddf4638572d<br/>
```
否则，我就得先用docker inspect命令查看容器的IP地址，然后访问“http://<容器IP地址>:80”才可以看到容器内应用的返回。
## 容器执行
>docker exec是怎么做到进入容器里的呢？

这个容器运行起来后，我又在里面做了一些操作，并且要把操作结果保存到镜像里，比如：

```
$ docker exec -it 4ddf4638572d /bin/sh
# 在容器内部新建了一个文件
root@4ddf4638572d:/app# touch test.txt
root@4ddf4638572d:/app# exit

#将这个新建的文件提交到镜像中保存
$ docker commit 4ddf4638572d geektime/helloworld:v2
```
Linux Namespace创建的隔离空间虽然看不见摸不着，但一个进程的Namespace信息在宿主机上是确确实实存在的，并且是以一个文件的方式存在。

比如，通过如下指令，你可以看到当前正在运行的Docker容器的进程号（PID）是25686：


```
$ docker inspect --format '{{ .State.Pid }}'  4ddf4638572d
25686
```
这时，你可以通过查看宿主机的proc文件，看到这个25686进程的所有Namespace对应的文件：


```
$ ls -l  /proc/25686/ns
total 0
lrwxrwxrwx 1 root root 0 Aug 13 14:05 cgroup -> cgroup:[4026531835]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 ipc -> ipc:[4026532278]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 mnt -> mnt:[4026532276]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 net -> net:[4026532281]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 pid -> pid:[4026532279]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 pid_for_children -> pid:[4026532279]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 uts -> uts:[4026532277]
```
可以看到，一个进程的每种Linux Namespace，都在它对应的/proc/[进程号]/ns下有一个对应的虚拟文件，并且链接到一个真实的Namespace文件上。

有了这样一个可以“hold住”所有Linux Namespace的文件，我们就可以对Namespace做一些很有意义事情了，比如：加入到一个已经存在的Namespace当中。

这也就意味着：一个进程，可以选择加入到某个进程已有的Namespace当中，从而达到“进入”这个进程所在容器的目的，这正是docker exec的实现原理。

而这个操作所依赖的，乃是一个名叫setns()的Linux系统调用。
### eg:
```bash
#define _GNU_SOURCE
#include <fcntl.h>
#include <sched.h>
#include <unistd.h>
#include <stdlib.h>
#include <stdio.h>

#define errExit(msg) do { perror(msg); exit(EXIT_FAILURE);} while (0)

int main(int argc, char *argv[]) {
    int fd;
    
    fd = open(argv[1], O_RDONLY);
    if (setns(fd, 0) == -1) {
        errExit("setns");
    }
    execvp(argv[2], &argv[2]); 
    errExit("execvp");
}
```
这段代码功能非常简单：它一共接收两个参数，第一个参数是argv[1]，即当前进程要加入的Namespace文件的路径，比如/proc/25686/ns/net；而第二个参数，则是你要在这个Namespace里运行的进程，比如/bin/bash。

这段代码的核心操作，则是通过open()系统调用打开了指定的Namespace文件，并把这个文件的描述符fd交给setns()使用。在setns()执行后，当前进程就加入了这个文件对应的Linux Namespace当中了。

现在，你可以编译执行一下这个程序，加入到容器进程（PID=25686）的Network Namespace中：
```bash
$ gcc -o set_ns set_ns.c 
$ ./set_ns /proc/25686/ns/net /bin/bash 
$ ifconfig
eth0      Link encap:Ethernet  HWaddr 02:42:ac:11:00:02  
          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:12 errors:0 dropped:0 overruns:0 frame:0
          TX packets:10 errors:0 dropped:0 overruns:0 carrier:0
	   collisions:0 txqueuelen:0 
          RX bytes:976 (976.0 B)  TX bytes:796 (796.0 B)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
	  collisions:0 txqueuelen:1000 
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
```
正如上所示，当我们执行ifconfig命令查看网络设备时，我会发现能看到的网卡“变少”了：只有两个。而我的宿主机则至少有四个网卡。这是怎么回事呢？

实际上，在setns()之后我看到的这两个网卡，正是我在前面启动的Docker容器里的网卡。也就是说，我新创建的这个/bin/bash进程，由于加入了该容器进程（PID=25686）的Network Namepace，它看到的网络设备与这个容器里是一样的，即：/bin/bash进程的网络设备视图，也被修改了。

而一旦一个进程加入到了另一个Namespace当中，在宿主机的Namespace文件上，也会有所体现。

在宿主机上，你可以用ps指令找到这个set_ns程序执行的/bin/bash进程，其真实的PID是28499：


```
# 在宿主机上
ps aux | grep /bin/bash
root     28499  0.0  0.0 19944  3612 pts/0    S    14:15   0:00 /bin/bash
```
这时，如果按照前面介绍过的方法，查看一下这个PID=28499的进程的Namespace，你就会发现这样一个事实：


```
$ ls -l /proc/28499/ns/net
lrwxrwxrwx 1 root root 0 Aug 13 14:18 /proc/28499/ns/net -> net:[4026532281]

$ ls -l  /proc/25686/ns/net
lrwxrwxrwx 1 root root 0 Aug 13 14:05 /proc/25686/ns/net -> net:[4026532281]
```
在/proc/[PID]/ns/net目录下，这个PID=28499进程，与我们前面的Docker容器进程（PID=25686）指向的Network Namespace文件完全一样。这说明这两个进程，共享了这个名叫net:[4026532281]的Network Namespace。
## 数据卷
>容器里进程新建的文件，怎么才能让宿主机获取到？宿主机上的文件和目录，怎么才能让容器里的进程访问到？

Volume机制，允许你将宿主机上指定的目录或者文件，挂载到容器里面进行读取和修改操作。
当容器进程被创建之后，尽管开启了Mount Namespace，但是在它执行chroot（或者pivot_root）之前，容器进程一直可以看到宿主机上的整个文件系统。

而宿主机上的文件系统，也自然包括了我们要使用的容器镜像。这个镜像的各个层，保存在/var/lib/docker/aufs/diff目录下，在容器进程启动后，它们会被联合挂载在/var/lib/docker/aufs/mnt/目录中，这样容器所需的rootfs就准备好了。

所以，我们只需要在rootfs准备好之后，在执行chroot之前，把Volume指定的宿主机目录（比如/home目录），挂载到指定的容器目录（比如/test目录）在宿主机上对应的目录（即/var/lib/docker/aufs/mnt/[可读写层ID]/test）上，这个Volume的挂载工作就完成了。
由于执行这个挂载操作时，“容器进程”已经创建了，也就意味着此时Mount Namespace已经开启了。所以，这个挂载事件只在这个容器里可见。你在宿主机上，是看不见容器内部的这个挂载点的。这就保证了容器的隔离性不会被Volume打破。

>注意：这里提到的"容器进程"，是Docker创建的一个容器初始化进程(dockerinit)，而不是应用进程(ENTRYPOINT + CMD)。dockerinit会负责完成根目录的准备、挂载设备和目录、配置hostname等一系列需要在容器内进行的初始化操作。最后，它通过execv()系统调用，让应用进程取代自己，成为容器里的PID=1的进程。

Linux的绑定挂载（bind mount）机制，允许你将一个目录或者文件，而不是整个设备，挂载到一个指定的目录上。并且，这时你在该挂载点上进行的任何操作，只是发生在被挂载的目录或者文件上，而原挂载点的内容则会被隐藏起来且不受影响。绑定挂载实际上是一个inode替换的过程。在Linux操作系统中，inode可以理解为存放文件内容的“对象”，而dentry，也叫目录项，就是访问这个inode所使用的“指针”。
![](./images/1725363058397_image.png)
在一个正确的时机，进行一次绑定挂载，Docker就可以成功地将一个宿主机上的目录或文件，不动声色地挂载到容器中。

这样，进程在容器里对这个/test目录进行的所有操作，都实际发生在宿主机的对应目录（比如，/home，或者/var/lib/docker/volumes/[VOLUME_ID]/_data）里，而不会影响容器镜像的内容。
>那么，这个/test目录里的内容，既然挂载在容器rootfs的可读写层，它会不会被docker commit提交掉呢？
也不会。容器的镜像操作，比如docker commit，都是发生在宿主机空间的。而由于Mount Namespace的隔离作用，宿主机并不知道这个绑定挂载的存在。所以，在宿主机看来，容器中可读写层的/test目录（/var/lib/docker/aufs/mnt/[可读写层ID]/test），始终是空的。
### eg:
首先，启动一个helloworld容器，给它声明一个Volume，挂载在容器里的/test目录上：


```
$ docker run -d -v /test helloworld
cf53b766fa6f
```
容器启动之后，我们来查看一下这个Volume的ID：


```
$ docker volume ls
DRIVER              VOLUME NAME
local               cb1c2f7221fa9b0971cc35f68aa1034824755ac44a034c0c0a1dd318838d3a6d
```
然后，使用这个ID，可以找到它在Docker工作目录下的volumes路径：

```
$ ls /var/lib/docker/volumes/cb1c2f7221fa/_data/
```
这个_data文件夹，就是这个容器的Volume在宿主机上对应的临时目录了。

接下来，我们在容器的Volume里，添加一个文件text.txt：


```
$ docker exec -it cf53b766fa6f /bin/sh
cd test/
touch text.txt
```
这时，我们再回到宿主机，就会发现text.txt已经出现在了宿主机上对应的临时目录里：


```
$ ls /var/lib/docker/volumes/cb1c2f7221fa/_data/
text.txt
```
可是，如果你在宿主机上查看该容器的可读写层，虽然可以看到这个/test目录，但其内容是空的（关于如何找到这个AuFS文件系统的路径，请参考我上一次分享的内容）：

```
$ ls /var/lib/docker/aufs/mnt/6780d0778b8a/test
```
可以确认，容器Volume里的信息，并不会被docker commit提交掉；但这个挂载点目录/test本身，则会出现在新的镜像当中。
