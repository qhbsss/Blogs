[TOC]



# 服务拆分

## 纵向拆分

从业务维度进⾏拆分。标准是按照业务的关联程度来决定，关联⽐较密切的业务适合拆分为⼀个微服务，⽽功能相对⽐较独⽴的业务适合单独拆分为⼀个微服务。

将不同的功能模块服务化，独⽴部署和运维。以社交App为例，你可以认为⾸⻚信息流是⼀个服务，评论是⼀个服务，消息通知是⼀个服务，个⼈主⻚也是⼀个服务。

## 横向拆分

从公共且独⽴功能维度拆分。标准是按照是否有公共的被多个其他服务调⽤，且依赖的资源独⽴不与其他业务耦合。

以社交App举例，⽆论是⾸⻚信息流、评论、消息箱还是个⼈主⻚，都需要显示⽤户的昵称。假如⽤户的昵称功能有产品需求的变更，你需要上线⼏乎所有的服务，这个成本就有点⾼了。显⽽易⻅，如果我把⽤户的昵称功能单独部署成⼀个独⽴的服务，那么有什么变更我只需要上线这个服务即可，其他服务不受影响，开发和上线成本就⼤⼤降低了。

# 服务化需要解决的问题

## 1. 服务如何定义
对于单体应⽤来说，不同功能模块之前相互交互时，通常是以类库的⽅式来提供各个模块的功能。对于微服务来说，每个服务都运⾏在各⾃的进程之中，应该以何种形式向外界传达⾃⼰的信息呢？答案就是接⼝，⽆论采⽤哪种通讯协议，是HTTP还是RPC，服务之间的调⽤都通过接⼝描述来约定，约定内容包括接⼝名、接⼝参数以及接⼝返回值。
## 2. 服务如何发布和订阅
单体应⽤由于部署在同⼀个WAR包⾥，接⼝之间的调⽤属于进程内的调⽤。⽽拆分为微服务独⽴部署后，服务提供者该如何对外暴露⾃⼰的地址，服务调⽤者该如何查询所需要调⽤的服务的地呢？这个时候你就需要⼀个类似登记处的地⽅，能够记录每个服务提供者的地址以供服务调⽤者查询，在微服务架构⾥，这个地⽅就是注册中⼼。
## 3. 服务如何监控
通常对于⼀个服务，我们最关⼼的是QPS（调⽤量）、AvgTime（平均耗时）以及P999（99.9%的请求性能在多少毫秒以内）这些指标。这时候你就需要⼀种通⽤的监控⽅案，能够覆盖业务埋点、数据收集、数据处理，最后到数据展示的全链路功能。
## 4. 服务如何治理
可以想象，拆分为微服务架构后，服务的数量变多了，依赖关系也变复杂了。⽐如⼀个服务的性能有问题时，依赖的服务都势必会受到影响。可以设定⼀个调⽤性能阈值，如果⼀段时间内⼀直超过这个值，那么依赖服务的调⽤可以直接返回，这就是熔断，也是服务治理最常⽤的⼿段之⼀。
## 5. 故障如何定位
在单体应⽤拆分为微服务之后，⼀次⽤户调⽤可能依赖多个服务，每个服务⼜部署在不同的节点上，如果⽤户调⽤出现问题，你需要有⼀种解决⽅案能够将⼀次⽤户请求进⾏标记，并在多个依赖的服务系统中继续传递，以便串联所有路径，从⽽进⾏故障定位。

# 微服务框架

![image-20241022180042112](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesimage-20241022180042112.png)

## 服务描述

服务调⽤⾸先要解决的问题就是服务如何对外描述。比如，你对外提供了⼀个服务，那么这个服务的服务名叫什么？调⽤这个服务需要提供哪些信息？调⽤这个服务返回的结果是什么格式的？该如何解析？这些就是服务描述要解决的问题。
常⽤的服务描述⽅式包括RESTful API、XML配置以及IDL文件三种。

![image-20241023114203228](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesimage-20241023114203228.png)

### RESTful API
RESTful API的⽅式，主要被⽤作HTTP或者HTTPS协议的接⼝定义，即使在⾮微服务架构体系下，也被⼴泛采⽤。

### XML配置
接下来再来给你讲下XML配置⽅式，这种⽅式的服务发布和引⽤主要分三个步骤：

- 服务提供者定义接⼝，并实现接⼝。
- 服务提供者进程启动时，通过加载server.xml配置⽂件将接⼝暴露出去。
- 服务消费者进程启动时，通过加载client.xml配置⽂件来引⼊要调⽤的接⼝。

通过在服务提供者和服务消费者之间维持⼀份对等的XML配置⽂件，来保证服务消费者按照服务提供者的约定来进⾏服务调⽤。在这种⽅式下，如果服务提供者变更了接⼝定义，不仅需要更新服务提供者加载的接⼝描述⽂件server.xml，还需要同时更新服务消费者加载的接⼝描述⽂件client.xml。

### IDL文件

IDL就是接⼝描述语⾔（interface description language）的缩写，通过⼀种中⽴的⽅式来描述接⼝，使得在不同的平台上运⾏的对象和不同语⾔编写的程序可以相互通信交流。⽐如你⽤Java语⾔实现提供的⼀个服务，也能被PHP语⾔调⽤。
也就是说IDL主要是⽤作跨语⾔平台的服务之间的调⽤，有两种最常⽤的IDL：⼀个是Facebook开源的Thrift协议，另⼀个是Google开源的gRPC协议。⽆论是Thrift协议还是gRPC协议，它们的⼯作原理都是类似的。

## 注册中心
有了服务的接口描述，下⼀步要解决的问题就是服务的发布和订阅，就是说你提供了⼀个服务，如何让外部想调⽤你的服务的人知道。这个时候就需要⼀个类似注册中心的角色，服务提供者将⾃⼰提供的服务以及地址登记到注册中心，服务消费者则从注册中心查询所需要调⽤的服务的地址，然后发起请求。

![image-20241023111220043](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20241023111220043.png)

## 服务框架
通过注册中心，服务消费者就可以获取到服务提供者的地址，有了地址后就可以发起调⽤。但在发起调⽤之前你还需要解决以下几个问题。

- 服务通信采⽤什么协议？就是说服务提供者和服务消费者之间以什么样的协议进⾏⽹络通信，是采⽤四层TCP、UDP协
  议，还是采⽤七层HTTP协议，还是采⽤其他协议？
- 数据传输采⽤什么⽅式？就是说服务提供者和服务消费者之间的数据传输采⽤哪种⽅式，是同步还是异步，是在单连接上
  传输，还是多路复⽤。
- 数据压缩采⽤什么格式？通常数据传输都会对数据进⾏压缩，来减少⽹络传输的数据量，从⽽减少带宽消耗和⽹络传输时
  间，⽐如常⻅的JSON序列化、Java对象序列化以及Protobuf序列化等。

## 服务监控
⼀旦服务消费者与服务提供者之间能够正常发起服务调⽤，你就需要对调⽤情况进⾏监控，以了解服务是否正常。通常来讲，服务监控主要包括三个流程。

- 指标收集。就是要把每⼀次服务调⽤的请求耗时以及成功与否收集起来，并上传到集中的数据处理中⼼。
- 数据处理。有了每次调⽤的请求耗时以及成功与否等信息，就可以计算每秒服务请求量、平均耗时以及成功率等指标。
- 数据展示。数据收集起来，经过处理之后，还需要以友好的⽅式对外展示，才能发挥价值。通常都是将数据展示在
  Dashboard⾯板上，并且每隔10s等间隔⾃动刷新，⽤作业务监控和报警等。

## 服务追踪
除了需要对服务调⽤情况进⾏监控之外，你还需要记录服务调⽤经过的每⼀层链路，以便进⾏问题追踪和故障定位。
服务追踪的⼯作原理⼤致如下：

- 服务消费者发起调⽤前，会在本地按照⼀定的规则⽣成⼀个requestid，发起调⽤时，将requestid当作请求参数的⼀部分，
  传递给服务提供者。
- 服务提供者接收到请求后，记录下这次请求的requestid，然后处理请求。如果服务提供者继续请求其他服务，会在本地再
  ⽣成⼀个⾃⼰的requestid，然后把这两个requestid都当作请求参数继续往下传递。

## 服务治理
服务监控能够发现问题，服务追踪能够定位问题所在，⽽解决问题就得靠服务治理了。服务治理就是通过⼀系列的⼿段来保证
在各种意外情况下，服务调⽤仍然能够正常进⾏。在⽣产环境中，你应该经常会遇到下⾯⼏种状况。

- 单机故障。通常遇到单机故障，都是靠运维发现并重启服务或者从线上摘除故障节点。然⽽集群的规模越⼤，越是容易遇
  到单机故障，在机器规模超过⼀百台以上时，靠传统的⼈⾁运维显然难以应对。⽽服务治理可以通过⼀定的策略，⾃动摘
  除故障节点，不需要⼈为⼲预，就能保证单机故障不会影响业务。
- 单IDC故障。你应该经常听说某某App，因为施⼯挖断光缆导致⼤批量⽤户⽆法使⽤的严重故障。⽽服务治理可以通过⾃动
  切换故障IDC的流量到其他正常IDC，可以避免因为单IDC故障引起的⼤批量业务受影响。
- 依赖服务不可⽤。⽐如你的服务依赖依赖了另⼀个服务，当另⼀个服务出现问题时，会拖慢甚⾄拖垮你的服务。⽽服务治
  理可以通过熔断，在依赖服务异常的情况下，⼀段时期内停⽌发起调⽤⽽直接返回。这样⼀⽅⾯保证了服务消费者能够不
  被拖垮，另⼀⽅⾯也给服务提供者减少压⼒，使其能够尽快恢复。
# 微服务落地
## DevOps

### 微服务带来的问题

单体应⽤拆分成多个微服务后，能够实现快速开发迭代，但随之带来的问题是测试和运维部署的成本的提升。相信拆分微服务的利弊你早已⽿熟能详，我讲个具体的例⼦。微博业务早期就是⼀个⼤的单体Web应⽤，在测试和运维的时候，只需要把Web应⽤打成⼀个⼤的WAR包，部署到Tomcat中去就⾏了。后来拆分成多个微服务之后，有的业务需求需要同时修改多个微服务的代码，这时候就有多个微服务都需要打包、测试和上线发布，⼀个业务需求就需要同时测试多个微服务接⼝的功能，上线发布多个系统，给测试和运维的⼯作量增加了很多。这个时候就需要有办法能够减轻测试和运维的负担，我在上⼀讲给出的解决⽅案是DevOps。
DevOps可以简单理解为开发和运维的结合，服务的开发者不再只负责服务的代码开发，还要负责服务的测试、上线发布甚⾄故障处理等全⽣命周期过程，这样的话就把测试和运维从微服务拆分后所带来的复杂⼯作中解放出来。DevOps要求开发、测试和发布的流程必须⾃动化，这就需要保证开发⼈员将⾃⼰本地部署测试通过的代码和运⾏环境，能够复制到测试环境中去，测试通过后再复制到线上环境进⾏发布。虽然这个过程看上去好像复制代码⼀样简单，但在现实时，本地环境、测试环境以及线上环境往往是隔离的，软件配置环境的差异也很⼤，这也导致了开发、测试和发布流程的割裂。
### Docker
Docker镜像解决了DevOps中微服务运⾏的环境难以在本地环境、测试环境以及线上环境保持⼀致的难题。如此⼀来，开发就可以把在本地环境中运⾏测试通过的代码，以及依赖的软件和操作系统本身打包成⼀个镜像，然后⾃动部署在测试环境中进⾏测试，测试通过后再⾃动发布到线上环境上去，整个开发、测试和发布的流程就打通了。
实际在使⽤Docker镜像的时候往往并不是
把业务代码、依赖的软件环境以及操作系统本身直接都打包成⼀个镜像，⽽是利⽤Docker镜像的分层机制，在每⼀层通过编写Dockerfile⽂件来逐层打包镜像。这是因为虽然不同的微服务依赖的软件环境不同，但是还是存在⼤⼤⼩⼩的相同之处，因此在打包Docker镜像的时候，可以分层设计、逐层复⽤，这样的话可以减少每⼀层镜像⽂件的⼤⼩。

## 微服务容器化的运维
>业务容器化后，运维⾯对的不再是⼀台台实实在在的物理机或者虚拟机了，⽽是⼀个个Docker容器，它们可能都没有固定的IP.
>这时候就需要⼀个⾯向容器的新型运维平台，它能够在现有的物理机或者虚拟机上创建容器，并且能够像运维物理机或者虚拟机⼀样，对容器的⽣命周期进⾏管理，通常我们叫它“容器运维平台”。
⼀个容器运维平台通常包含以下⼏个组成部分：镜像仓库、资源调度、容器调度和服务编排。

### 镜像仓库

镜像仓库的概念其实跟Git代码仓库类似，就是有⼀个集中存储的地⽅，把镜像存储在这⾥，在服务发布的时候，各个服务器都访问这个集中存储来拉取镜像，然后启动容器。

#### 1. 权限控制
镜像仓库⾸先⾯临的第⼀个问题就是权限控制的问题，也就是说哪些⽤户可以拉取镜像，哪些⽤户可以修改镜像。
⼀般来说，镜像仓库都设有两层权限控制：⼀是必须登录才可以访问，这是最外层的控制，它规定了哪些⼈可以访问镜像仓库；⼆是对镜像按照项⽬的⽅式进⾏划分，每个项⽬拥有⾃⼰的镜像仓库⽬录，并且给每个项⽬设置项⽬管理员、开发者以及客⼈这三个⻆⾊，只有项⽬管理员和开发者拥有⾃⼰镜像仓库⽬录下镜像的修改权限，⽽客⼈只拥有访问权限，项⽬管理员可以给这个项⽬设置哪些⼈是开发者。

#### 2. 镜像同步
在实际的⽣产环境中，往往需要把镜像同时发布到⼏⼗台或者上百台集群节点上，单个镜像仓库实例往往受带宽原因限制⽆法同时满⾜⼤量节点的下载需求，这个时候就需要配置多个镜像仓库实例来做负载均衡，同时也就产⽣镜像在多个镜像仓库实例之间同步的问题了。显然通过⼿⼯维护⼗分繁琐，那有什么好的办法吗？
⼀般来说，有两种⽅案，⼀种是⼀主多从，主从复制的⽅案，⽐如开源镜像仓库Harbor采⽤了这种⽅案；另⼀种是P2P的⽅案，⽐如阿⾥的容器镜像分发系统蜻蜓采⽤了P2P⽅案。

![image-20241027205408578](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesimage-20241027205408578.png)

#### 3. ⾼可⽤性
既然Docker镜像是Docker容器运⾏的基础，那么镜像仓库的⾼可⽤性就不⾔⽽喻了。⼀般⽽⾔，⾼可⽤性设计⽆⾮就是把服务部署在多个IDC，这样的话即使有IDC出问题，也可以把服务迁移到别的正常IDC中去。同样对于镜像仓库的搭建，也可以采⽤多IDC部署，那么需要做到的就是不同IDC之间的镜像同步。

### 资源调度
解决了Docker镜像存储和访问的问题后，新问题⼜随之⽽来了，Docker镜像要分发到哪些机器上去？这些机器是从哪⾥来的？这其实涉及的是资源调度的问题。
根据我的经验，服务部署的集群主要包括三种：

1. 物理机集群。⼤部分中⼩团队应该都拥有⾃⼰的物理机集群，并且⼤多按照集群 - 服务池 - 服务器这种模式进⾏运维。物理机集群⾯临的问题，主要是服务器的配置不统⼀，尤其对于计算节点来说，普遍存在的⼀种情况就是⼏年前采购的机器的配置可能还是12核16G内存的配置，⽽近些年采购的机器都⾄少是32核32G内存的配置，对于这两种机器往往要区别对待，⽐如旧的机器⽤于跑⼀些⾮核⼼占⽤资源量不⼤的业务，⽽新采购的机器⽤于跑⼀些核⼼且服务调⽤量⾼的业务。
2. 虚拟机集群。不少业务团队在使⽤物理机集群之后，发现物理机集群存在使⽤率不⾼、业务迁移不灵活的问题，因此纷纷转向了虚拟化⽅向，构建⾃⼰的私有云，⽐如以OpenStack技术为主的私有云集群在国内外不少业务团队都有⼤规模的应⽤。它的最⼤好处就是可以整合企业内部的服务器资源，通过虚拟化技术进⾏按需分配，提⾼集群的资源使⽤率，节省成本。
3. 公有云集群。现在越来越多的业务团队，尤其是初创公司，因为公有云快速灵活的特性，纷纷在公有云上搭建⾃⼰的业务。公有云最⼤的好处除了快速灵活、分钟级即可实现上百台机器的创建，还有个好处就是配置统⼀、便于管理，不存在机器配置碎⽚化问题。

### 容器调度
容器调度的问题，说的是现在集群⾥有⼀批可⽤的物理机或者虚拟机，当服务需要发布的时候，该选择哪些机器部署容器的问题。

这时就需要有专⻔的容器调度系统了，为此也诞⽣了不少基于Docker的容器调度系统，⽐如Docker原⽣的调度系统Swarm、Mesosphere出品的Mesos，以及Google开源的⼤名鼎鼎的Kubernetes。

#### 1. 主机过滤
主机过滤是为了解决容器创建时什么样的机器可以使⽤的问题，主要包含两种过滤。

- 存活过滤。也就是说必须选择存活的节点，因为主机也有可能下线或者是故障状态。
- 硬件过滤。打个⽐⽅，现在你⾯对的集群有Web集群、RPC集群、缓存集群以及⼤数据集群等，不同的集群硬件配置差异很⼤，⽐如Web集群往往⽤作计算节点，它的CPU⼀般配置⽐较⾼；⽽⼤数据集群往往⽤作数据存储，它的磁盘⼀般配置⽐较⾼。这样的话如果要创建计算任务的容器，显然就需要选择Web集群，⽽不是⼤数据集群。

### 服务编排
#### 1. 服务依赖
⼤部分情况下，微服务之间是相互独⽴的，在进⾏容器调度的时候不需要考虑彼此。但有时候也会存在⼀些场景，⽐如服务A调度的前提必须是先有服务B，这样的话就要求在进⾏容器调度的时候，还需要考虑服务之间的依赖关系。

#### 2. 服务发现
容器调度完成以后，容器就可以启动了，但此时容器还不能对外提供服务，服务消费者并不知道这个新的节点，所以必须具备服务发现机制，使得新的容器节点能够加⼊到线上服务中去。
#### 3. ⾃动扩缩容
容器完成调度后，仅仅做到有容器不可⽤时故障⾃愈还不够，有时候还需要根据实际服务的运⾏状况，做到⾃动扩缩容。
常⻅的⾃动扩缩容的做法是根据容器的CPU负载情况来设置⼀个扩缩容的容器数量或者⽐例，⽐如可以设定容器的CPU使⽤率不超过50%，⼀旦超过这个使⽤率就扩容⼀倍的机器。

## 微服务如何实现DevOps

要实现DevOps，就必须开发完成代码开发后，能⾃动进⾏测试，测试通过后，能⾃动发布到线上。对应的这两个过程就是CI和CD，具体来讲就是：

- CI（Continuous Integration），持续集成。开发完成代码开发后，能⾃动地进⾏代码检查、单元测试、打包部署到测试环境，进⾏集成测试，跑⾃动化测试⽤例。
- CD（Continuous Deploy），持续部署。代码测试通过后，能⾃动部署到类⽣产环境中进⾏集成测试，测试通过后再进⾏⼩流量的灰度验证，验证通过后代码就达到线上发布的要求了，就可以把代码⾃动部署到线上。

![image-20241027212447120](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesimage-20241027212447120.png)

⼀个服务的发布流程主要包含了三个步骤。

1. 持续集成，这个步骤的主要作⽤是确保每⼀次代码的Merge Request都测试通过，可随时合并到代码的Develop分⽀，主要
   包括四个阶段：build阶段（开发分⽀代码的编译与单元测试）、package阶段（开发分⽀代码打包成Docker镜像）、deploy阶
   段（开发分⽀代码部署到测试环境）、test阶段（开发分⽀代码集成测试）。

   >持续集成阶段的主要⽬的是保证每⼀次开发的代码都没有问题，即使合并到主⼲也能正常⼯作，这⾥主要依靠三部分的作⽤。
   >
   >- 代码检查。通过代码检查可以发现代码潜在的⼀些bug，⽐如Java对象有可能是null空指针等，实际执⾏时可以在持续集成
   >  阶段集成类似Sonarqube之类的⼯具来实现代码检查。
   >
   >- 单元测试。单元测试是保证代码运⾏质量的第⼆个关卡。单元测试是针对每个具体代码模块的，单元测试的覆盖度越⾼，
   >  各个代码模块出错的概率就越⼩。不过实际业务开发过程中，为了追求开发速度，许多开发者并不在意单元测试的覆盖
   >   度，⽽是把⼤部分测试⼯作都留在了集成测试阶段，这样可能会造成集成测试阶段返⼯的次数太多，需要多次修复bug才能
   >   通过集成测试。尤其对于业务复杂度⽐较⾼的服务来说，在单元测试阶段多花费⼀些功夫，其实从整个代码开发周期⻆度
   >   来看，收益还是要远⼤于付出的。
   >- 集成测试。集成测试就是将各个代码的修改集成到⼀起，统⼀部署在测试环境中进⾏测试。为了实现整个流程的⾃动化，
   >  集成⾃测阶段主要的任务就是跑每个服务的⾃动化测试⽤例，所以⾃动化测试⽤例覆盖的越全，集成测试的可靠性就越
   >   ⾼。这⾥就要求开发和测试能及时沟通，在新的业务需求确定时，就开始编写测试⽤例，这样在跑⾃动化测试⽤例时，就
   >   不需要测试的介⼊了，省去了沟通成本。当然，业务开发⼈员也可以⾃⼰编写测试⽤例，这样的话就不需要专职的业务测
   >   试⼈员了。

2. 持续交付，这个步骤的主要作⽤是确保所有代码合并Merage Request到Develop分⽀后，Develop分⽀的代码能够在⽣产环
   境中测试通过，并进⾏⼩流量灰度验证，可随时交付到线上。主要包括五个阶段：build阶段（Develop分⽀的代码编译与单元
   测试）、package阶段（Develop分⽀的代码打包成Docker镜像）、deploy阶段（Develop分⽀的代码部署到测试环境）、test
   阶段（Develop分⽀的代码集成测试）、canary阶段（Develop分⽀的代码的⼩流量灰度验证）。

   >持续交付阶段的主要⽬的是保证最新的业务代码，能够在类⽣产环境中可能够正常运⾏，⼀般做法都是从线上⽣成环境中摘掉
   >两个节点，然后在这两个节点上部署最新的业务代码，再进⾏集成测试，集成测试通过后再引⼊线上流量，来观察服务是否正
   >常。通常需要解决两个问题：
   >
   >- 如何从线上⽣产环境中摘除两个节点。这就需要接⼊线上的容器管理平台，⽐如微博的容器管理平台DCP就提供了类似下
   >  ⾯的API，能够从线上⽣产环境中摘除某个节点，然后部署最新的业务代码。
   >- 如何观察服务是否正常。由于这两个节点上运⾏的代码是最新的代码，在引⼊线上流量后可能会出现内存泄露等在集成测
   >  试阶段⽆法发现的问题，所以这个阶段这两个节点上运⾏最新代码后的状态必须与线上其他节点⼀致。实际观察时，主要
   >   有两个⼿段，⼀个是观察节点本身的状态，如CPU、内存、I/O、⽹卡等，⼀个是观察业务运⾏产⽣的warn、error的⽇志量
   >   的⼤⼩，尤其是error⽇志量有异常时，往往就说明最新的代码可能存在异常，需要处理后才能发布到线上。

3. 持续部署，这个步骤的主要作⽤是合并Develop分⽀到Master主⼲，并打包成Docker镜像，可随时发布到线上。主要包括四
   个阶段：build阶段（Master主⼲的代码编译与单元测试）、package阶段（Master主⼲的代码打包成Docker镜像）、clear阶
   段（Master主⼲的代码Merge回Develop分⽀）、production阶段（Master主⼲的代码发布到线上）。
   >持续部署阶段的主要⽬的把在类⽣产环境下运⾏通过的代码⾃动的发布到线上所有节点中去，这⾥的关键点就在于实际的线上
   >
   >发布阶段并不是想象中的那么直接。以微博API的业务为例，同样的服务也分为核⼼池和⾮核⼼池，核⼼池提供给移动端和PC
   >调⽤，⾮核⼼池提供给其他内部业务调⽤，并且还按照机房分为不同的服务池，⽐如永丰机房服务池和⼟城机房服务池。实际
   >发布的时候，考虑到线上服务的稳定性，并不是说按照⼀定的步⻓，⾃动把所有服务池都发布了，⽽是先发布⾮核⼼池以及⼟
   >城机房的核⼼池，然后验证观察⼀段时间线上服务⼀切正常后，再继续发布永丰机房的核⼼池，以防⽌某些问题在服务发布的
   >过程中才暴露出来，但⼜不⾄于影响线上所有的服务节点。所以这个阶段，持续部署⼀般并不要求那么完美，许多公司在这个
   >阶段都采⽤了⼿动发布的⽅式以控制⻛险，或者只做到持续交付阶段，对于持续部署并不要求⾃动化。

