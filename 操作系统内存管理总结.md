[TOC]
# 虚拟内存

把进程所使用的地址「隔离」开来，即让操作系统为每个进程分配独立的一套「**虚拟地址**」，人人都有，大家自己玩自己的地址就行，互不干涉。但是有个前提每个进程都不能访问物理地址，至于虚拟地址最终怎么落到物理内存里，对进程来说是透明的，操作系统已经把这些都安排的明明白白了。

![进程的中间层](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures298fb68e3da94d767b02f2ed81ebf2c4.png)

**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。

于是，这里就引出了两种地址的概念：

- 我们程序所使用的内存地址叫做**虚拟内存地址**（*Virtual Memory Address*）
- 实际存在硬件里面的空间地址叫**物理内存地址**（*Physical Memory Address*）。

操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存，如下图所示：

![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_PicturesBlog_Pictures72ab76ba697e470b8ceb14d5fc5688d9.png)

# 内存碎片

内存碎片是由内存的申请和释放产生的，通常分为下面两种：

- **内部内存碎片(Internal Memory Fragmentation，简称为内存碎片)**：已经分配给进程使用但未被使用的内存。导致内部内存碎片的主要原因是，当采用固定比例比如 2 的幂次方进行内存分配时，进程所分配的内存可能会比其实际所需要的大。举个例子，一个进程只需要 65 字节的内存，但为其分配了 128（2^7） 大小的内存，那 63 字节的内存就成为了内部内存碎片。
- **外部内存碎片(External Memory Fragmentation，简称为外部碎片)**：由于未分配的连续内存区域太小，以至于不能满足任意进程所需要的内存分配请求，这些小片段且不连续的内存空间被称为外部碎片。也就是说，外部内存碎片指的是那些并未分配给进程但又不能使用的内存。我们后面介绍的分段机制就会导致外部内存碎片。

![内存碎片](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesinternal-and-external-fragmentation.png)

内存管理方式可以简单分为下面两种：

- **连续内存管理**：为**一个用户程序**分配一个连续的内存空间，内存利用率一般不高。
- **非连续内存管理**：允许**一个程序**使用的内存分布在离散或者说不相邻的内存中，相对更加灵活一些。

>linux整体上采用非连续内存的管理的段页式内存管理方式，但当内核需要连续的物理内存时，伙伴系统会尝试分配连续的块，但这只是内存管理的一个方面，而不是Linux整体内存管理的基础。

## 连续内存管理

**块式管理** 是早期计算机操作系统的一种连续内存管理方式，存在严重的内存碎片问题。块式管理会将内存分为几个固定大小的块，每个块中只包含一个进程。如果程序运行需要内存的话，操作系统就分配给它一块，如果程序运行只需要很小的空间的话，分配的这块内存很大一部分几乎被浪费了。这些在每个块中未被利用的空间，我们称之为内部内存碎片。除了内部内存碎片之外，由于两个内存块之间可能还会有外部内存碎片，这些不连续的外部内存碎片由于太小了无法再进行分配。

**动态分区分配策略：**

- 最先匹配：最先找到哪个满足条件的空闲分区就把进程分区放进去（一般性能最好）
- 最佳匹配：找最佳的空闲分区 ，使得空闲分区大小和进程分区大小接近
- 最差匹配：每次用的空闲分区都是最大的。

在操作系统层面，Linux系统使用伙伴系统Buddy分配器以页为单位来组织物理内存页框，对物理内存页进行合理的分配和回收，让内存分配与相邻内存合并能快速进行，用于缓解外部内存碎片，同时为了减少内部碎片，Linux还引入了slab算法，将内存页拆分为更小的单位来管理，slab可以对小对象空间进行分配，而不需要分配整个页面给对象，这样可以节省空间，内核中对于频繁使用的小对象，slab还会对此作缓存，避免频繁的内存分配和回收。

在 Linux 系统中，连续内存管理采用了 **伙伴系统（Buddy System）算法** 来实现，这是一种经典的连续内存分配算法，可以有效解决外部内存碎片的问题。伙伴系统的主要思想是将内存按 2 的幂次划分（每一块内存大小都是 2 的幂次比如 2^6=64 KB），并将相邻的内存块组合成一对伙伴（注意：**必须是相邻的才是伙伴**）。

> 伙伴系统工作在页式内存管理的基础之上
Linux使用分页机制管理物理内存的，把所有的空闲页分组为11个块链表，每个块链表分别包含大小为1，2，4，8，16，32，64，128，256，512和1024个连续页框的页块（2^0 ~ 2^10，页阶order从0-10）。最大可以申请1024个连续页，对应4MB大小的连续内存（待验证下）。每个页块的第一个页的物理地址是该块大小的整数倍。
Linux中的伙伴系统是以页面为最小单位分配的，工作机制如下：
当内核申请页阶为n的页块时，伙伴系统会先从块链表n中查找是否有空闲页；
如果没有则从大一阶的页块中查找（没找到继续往大页链表中找）；
找到后，将大页阶进行对半切割，一半用于内存分配，另一半放到对应页阶的空闲链表中，同时设置伙伴标记。）

当进行内存分配时，伙伴系统会尝试找到大小最合适的内存块。如果找到的内存块过大，就将其一分为二，分成两个大小相等的伙伴块。如果还是大的话，就继续切分，直到到达合适的大小为止。

假设两块相邻的内存块都被释放，系统会将这两个内存块合并，进而形成一个更大的内存块，以便后续的内存分配。这样就可以减少内存碎片的问题，提高内存利用率。

![伙伴系统（Buddy System）内存管理](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictureslinux-buddy-system.png)

虽然解决了外部内存碎片的问题，但伙伴系统仍然存在内存利用率不高的问题（内部内存碎片）。这主要是因为伙伴系统只能分配大小为 2^n 的内存块，因此当需要分配的内存大小不是 2^n 的整数倍时，会浪费一定的内存空间。举个例子：如果要分配 65 大小的内存快，依然需要分配 2^7=128 大小的内存块。

![伙伴系统内存浪费问题](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesbuddy-system-memory-waste.png)

对于内部内存碎片的问题，Linux 采用 **SLAB** 进行解决。
内核中对内存页的分配使用有两种方式，一种是一页一页的分配使用，这种以页为单位的分配方式内核会向相应内存区域zone里的伙伴系统申请以及释放。
另一种方式就是只分配小块的内存，不需要一下分配一页的内存，比如前边章节中提到的struct page ,anon_vma_chain , anon_vma ,vm_area_struct结构实例的分配，这些结构通常就是几十个字节大小，并不需要按页来分配。
为了满足类似这种小内存分配的需要，Linux内核使用slab allocator分配器来分配，slab就好比一个对象池，内核中的数据结构对象都对应于一个slab对象池，用于分配这些固定类型对象所需要的内存。
它的基本原理是从伙伴系统中申请一整页内存，然后划分成多个大小相等的小块内存被slab所管理。这样一来slab就和物理内存页page 发生了关联，由于slab管理的单元是物理内存页page内进一步划分出来的小块内存，所以当page被分配给相应slab结构之后，struct page里也会存放slab相关的一些管理数据。

## 非连续内存管理

非连续内存管理存在下面 3 种方式：

- **段式管理**：以段(—段连续的物理内存)的形式管理/分配物理内存。应用程序的虚拟地址空间被分为大小不等的段，段是有实际意义的，每个段定义了一组逻辑信息，例如有主程序段 MAIN、子程序段 X、数据段 D 及栈段 S 等。
- **页式管理**：把物理内存分为连续等长的物理页，应用程序的虚拟地址空间也被划分为连续等长的虚拟页，是现代操作系统广泛使用的一种内存管理方式。
- **段页式管理机制**：结合了段式管理和页式管理的一种内存管理机制，把物理内存先分成若干段，每个段又继续分成若干大小相等的页。

# 内存管理方式
## 内存分段
程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。**不同的段是有不同的属性的，所以就用分段（*Segmentation*）的形式把这些段分离出来。**

### 分段机制下，虚拟地址和物理地址的映射关系

分段机制下的虚拟地址由两部分组成，**段选择因子**和**段内偏移量**。

![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesa9ed979e2ed8414f9828767592aadc21.png)

段选择因子和段内偏移量：

- **段选择因子**就保存在段寄存器里面。段选择因子里面最重要的是**段号**，用作段表的索引。**段表**里面保存的是这个**段的基地址、段的界限和特权等级**等。

- 虚拟地址中的**段内偏移量**应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。

在上面，知道了虚拟地址是通过**段表**与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址，如下图：

![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesc5e2ab63e6ee4c8db575f3c7c9c85962.png)

### 分段的不足之处：
1. #### 内存碎片
内存分段管理可以做到段根据实际需求分配内存，所以有多少需求就分配多大的段，所以**不会出现内部内存碎片**。
但是由于每个段的长度不固定，所以多个段未必能恰好使用所有的内存空间，会产生了多个不连续的小物理内存，导致新的程序无法被装载，所以**会出现外部内存碎片**的问题。
解决「外部内存碎片」的问题就是**内存交换**。
2. #### 内存交换的效率低
对于多进程的系统来说，用分段的方式，外部内存碎片是很容易产生的，产生了外部内存碎片，那不得不重新 `Swap` 内存区域，这个过程会产生性能瓶颈。
因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。
所以，**如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。**

## 内存分页

分段的好处就是能产生连续的内存空间，但是会出现「外部内存碎片和内存交换的空间太大」的问题。

要解决这些问题，那么就要想出能少出现一些内存碎片的办法。另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，这样就可以解决问题了。这个办法，也就是**内存分页**（*Paging*）。

**分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小**。这样一个连续并且尺寸固定的内存空间，我们叫**页**（*Page*）。在 Linux 下，每一页的大小为 `4KB`。

虚拟地址与物理地址之间通过**页表**来映射，如下图：

![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures08a8e315fedc4a858060db5cb4a654af.png)


页表是存储在内存里的，**内存管理单元** （*MMU*）就做将虚拟内存地址转换成物理地址的工作。

而当进程访问的虚拟地址在页表中查不到时，系统会产生一个**缺页异常**，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。

### 分页机制下，虚拟地址和物理地址的映射关系

在分页机制下，虚拟地址分为两部分，**页号**和**页内偏移**。页号作为页表的索引，**页表**包含物理页每页所在**物理内存的基地址**，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。

![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures7884f4d8db4949f7a5bb4bbd0f452609.png)

总结一下，对于一个内存地址转换，其实就是这样三个步骤：

- 把虚拟内存地址，切分成页号和偏移量；
- 根据页号，从页表里面，查询对应的物理页号；
- 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

下面举个例子，虚拟内存中的页通过页表映射为了物理内存中的页，如下图：

![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures8f187878c809414ca2486b0b71e8880e.png)
#### 多级页表
简单的分页有空间上的缺陷，因为操作系统是可以同时运行非常多的进程的，这就意味着页表会非常的庞大。要解决上面的问题，就需要采用一种叫作**多级页表**（*Multi-Level Page Table*）的解决方案。

在前面我们知道了，对于单页表的实现方式，在 32 位和页大小 `4KB` 的环境下，一个进程的页表需要装下 100 多万个「页表项」，并且每个页表项是占用 4 字节大小的，于是相当于每个页表需占用 4MB 大小的空间。

我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 `1024` 个页表（二级页表），每个表（二级页表）中包含 `1024` 个「页表项」，形成**二级分页**。如下图所示：

![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures19296e249b2240c29f9c52be70f611d5.png)

根据计算机组成原理里面无处不在的**局部性原理**，**如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表**。

从页表的性质来看，保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址。假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以**页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项**（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。

把二级分页再推广到多级页表，就会发现页表占用的内存空间更少了，这一切都要归功于对局部性原理的充分应用。

对于 64 位的系统，两级分页肯定不够了，就变成了四级目录，分别是：

- 全局页目录项 PGD（*Page Global Directory*）；
- 上层页目录项 PUD（*Page Upper Directory*）；
- 中间页目录项 PMD（*Page Middle Directory*）；
- 页表项 PTE（*Page Table Entry*）；

![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures%E5%9B%9B%E7%BA%A7%E5%88%86%E9%A1%B5.png)


#### TLB

多级页表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了几道转换的工序，这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。

程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。

![](https://img-blog.csdnimg.cn/edce58534d9342ff89f5261b1929c754.png)


我们就可以利用这一特性，把最常访问的几个页表项存储到访问速度更快的硬件，于是计算机科学家们，就在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，这个 Cache 就是 TLB（*Translation Lookaside Buffer*） ，通常称为页表缓存、转址旁路缓存、快表等。

![](https://img-blog.csdnimg.cn/a3cdf27646b24614a64cfc5d7ccffa35.png)

在 CPU 芯片里面，封装了内存管理单元（*Memory Management Unit*）芯片，它用来完成地址转换和 TLB 的访问与交互。

有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。

TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。

### 内存分页的优缺点

内存分页由于内存空间都是预先划分好的，也就不会像内存分段一样，在段与段之间会产生间隙非常小的内存，这正是分段会产生外部内存碎片的原因。而**采用了分页，页与页之间是紧密排列的，所以不会有外部碎片。**

但是，因为内存分页机制分配内存的最小单位是一页，即使程序不足一页大小，我们最少只能分配一个页，所以页内会出现内存浪费，所以针对**内存分页机制会有内部内存碎片**的现象。

如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为**换出**（*Swap Out*）。一旦需要的时候，再加载进来，称为**换入**（*Swap In*）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，**内存交换的效率就相对比较高。**

![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures388a29f45fe947e5a49240e4eff13538.png)


更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是**只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。**

# 内存分配方式
## 空间空间分配方式的理论
此处讨论空闲空间管理的一些问题，无论是 malloc 库（管理进程中堆的页），还是操作系统本身（管理进程的地址空间），抽象出来说，都是空间空间管理的问题。

### 空闲空间管理的底层机制
大多数分配程序采用的通用机制都是空间分割与合并，如何快速并相对轻松地追踪已分配的空间，如何利用空闲区域的内部空间维护一个简单的链表，来追踪空闲和已分配的空间。

#### 空间分割与合并
空闲链表包含一组元素，记录了堆中的哪些空间还没有分配。假设有下面的 30 字节的堆：
[![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures2.png)](https://pic.taifua.com/Picture/website/ostep/c17/2.png)

这个堆对应的空闲链表会有两个元素，一个描述第一个 10 字节的空闲区域（字节 0～9），一个描述另一个空闲区域（字节 20～29）：

[![](https://pic.taifua.com/Picture/website/ostep/c17/3.png)](https://pic.taifua.com/Picture/website/ostep/c17/3.png)

通过上面的介绍可以看出，任何大于 10 字节的分配请求都会失败（返回 NULL），因为没有足够的连续可用空间。而恰好 10 字节的需求可以由两个空闲块中的任何一个满足。但是，如果申请小于 10 字节空间，会发生什么？

假设我们只申请一个字节的内存。这种情况下，**分配程序会执行所谓的分割动作：它找到一块可以满足请求的空闲空间，将其分割，第一块返回给用户，第二块留在空闲链表中**。在我们的例子中，假设这时遇到申请一个字节的请求，分配程序选择使用第二块空闲空间，对 malloc() 的调用会返回 20（1 字节分配区域的地址），空闲链表会变成这样：

[![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures4.png)](https://pic.taifua.com/Picture/website/ostep/c17/4.png)

从上面可以看出，空闲链表基本没有变化，只是第二个空闲区域的起始位置由 20 变成 21，长度由 10 变为 9 了。因此，如果请求的空间大小小于某块空闲块，分配程序通常会进行分割。

许多分配程序中因此也有一种机制，名为**合并**。还是看前面的例子（10 字节的空闲空间，10 字节的已分配空间，和另外 10 字节的空闲空间）。

对于这个（小）堆，如果应用程序调用 free(10)，归还堆中间的空间，会发生什么？如果只是简单地将这块空闲空间加入空闲链表，不多想想，可能得到如下的结果：

[![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures5.png)](https://pic.taifua.com/Picture/website/ostep/c17/5.png)

问题出现了：尽管整个堆现在完全空闲，但它似乎被分割成了 3 个 10 字节的区域。这时，如果用户请求 20 字节的空间，简单遍历空闲链表会找不到这样的空闲块，因此返回失败。

为了避免这个问题，分配程序会在释放一块内存时合并可用空间。想法很简单：**在归还一块空闲内存时，仔细查看要归还的内存块的地址以及邻近的空闲空间块。如果新归还的空间与一个原有空闲块相邻，就将它们合并为一个较大的空闲块**。通过合并，最后空闲链表应该像这样：

[![](https://pic.taifua.com/Picture/website/ostep/c17/6.png)](https://pic.taifua.com/Picture/website/ostep/c17/6.png)

实际上，这是堆的空闲链表最初的样子，在所有分配之前。通过合并，分配程序可以更好地确保大块的空闲空间能提供给应用程序。
#### 追踪已分配空间的大小

free(void *ptr) 接口没有块大小的参数。因此它假定对于给定的指针，内存分配库可以很快确定要释放空间的大小，从而将它放回空闲链表。

要完成这个任务，**大多数分配程序都会在头块中保存一点额外的信息，它在内存中，通常就在返回的内存块之前**。再看一个例子，如下图所示。在这个例子中，检查一个由 ptr 指着的 20 字节的已分配块，设想用户调用了 malloc()，并将结果保存在 ptr 中：ptr = malloc(20)。

[![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures7.png)](https://pic.taifua.com/Picture/website/ostep/c17/7.png)

该头块中至少包含所分配空间的大小（这个例子中是 20）。它也可能包含一些额外的指针来加速空间释放，**包含一个幻数（magic number）来提供完整性检查**，以及其他信息。这里假定一个简单的头块包含了分配空间的大小和一个幻数：

上面的例子看起来会像下图的样子。用户调用 free(ptr) 时，库会通过简单的指针运算得到头块的位置：

[![](https://pic.taifua.com/Picture/website/ostep/c17/8.png)](https://pic.taifua.com/Picture/website/ostep/c17/8.png)

获得头块的指针后，库可以很容易地确定幻数是否符合预期的值，作为正常性检查`(assert(hptr->magic == 1234567))`，并简单计算要释放的空间大小（即**头块的大小加区域长度**）。请注意前一句话中一个小但重要的细节：**实际释放的是头块大小加上分配给用户的空间的大小**。因此，如果用户请求 N 字节的内存，库不是寻找大小为 N 的空闲块，而是寻找 **N 加上头块大小的空闲块**。

#### 嵌入空闲链表

到目前为止，这个简单的空闲链表还只是一个概念上的存在，它就是一个**链表**，描述了堆中的空闲内存块。但如何在空闲内存内部建立这样一个链表呢？在更典型的链表中，如果要分配新节点，你会调用 malloc() 来获取该节点所需的空间。遗憾的是，在内存分配库内，**你需要在空闲空间本身中建立空闲空间链表**。

假设需要管理一个 4096 字节的内存块（即堆是 4KB）。为了将它作为一个空闲空间链表来管理，首先要初始化这个链表。开始，链表中只有一个条目，记录了大小为 4096 的空间（减去头块的大小）。下面是该链表中一个节点描述：

现在来看一些代码，它们初始化堆，并将空闲链表的第一个元素放在该空间中。假设堆构建在某块空闲空间上，这块空间通过系统调用 mmap() 获得。这不是构建这种堆的唯一选择，但在这个例子中很合适。下面是代码：

执行这段代码之后，链表的状态是它只有一个条目，记录大小为 4088。是的，这是一个小堆，但是一个很好的例子。head 指针指向这块区域的起始地址，假设是 16KB，如下图所示。

[![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures9.png)](https://pic.taifua.com/Picture/website/ostep/c17/9.png)

现在，假设有一个 100 字节的内存请求。为了满足这个请求，库首先要找到一个足够大小的块。因为只有一个 4088 字节的块，所以选中这个块。然后，这个块被分割为两块：一块足够满足请求（以及头块，如前所述），一块是剩余的空闲块。假设记录头块为 8 个字节（一个整数记录大小，一个整数记录幻数），堆中的空间如下图所示。

[![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures10.png)](https://pic.taifua.com/Picture/website/ostep/c17/10.png)

至此，对于 100 字节的请求，库从原有的一个空闲块中分配了 108 字节，返回指向它的一个指针（在上图中用 ptr 表示），并在其之前连续的 8 字节中记录头块信息，供未来的 free() 函数使用。同时将链表中的空闲节点缩小为 3980 字节（4088−108）。

现在再来看该堆，其中有 3 个已分配区域，每个 100（加上头块是 108）。这个堆如下图所示。

[![](https://pic.taifua.com/Picture/website/ostep/c17/11.png)](https://pic.taifua.com/Picture/website/ostep/c17/11.png)

可以看出，堆的前 324 字节已经分配，该空间中有 3 个头块，以及 3 个 100 字节的用户使用空间。空闲链表只有一个节点（由 head 指向），在 3 次分割后，现在大小只有 3764 字节。但如果用户程序通过 free() 归还一些内存，会发生什么？

在这个例子中，应用程序调用 free(16500)，归还了中间的一块已分配空间（内存块的起始地址 16384 加上前一块的 108，和这一块的头块的 8 字节，就得到了 16500）。这个值在前图中用 sptr 指向。

库马上弄清楚了这块要释放空间的大小，并将空闲块加回空闲链表。假设将它插入到空闲链表的头位置，该空间如下图所示。

[![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures12.png)](https://pic.taifua.com/Picture/website/ostep/c17/12.png)

现在的空闲链表包括一个小空闲块（100 字节，由链表的头指向）和一个大空闲块（3764 字节）。

链表终于有不止一个元素了！是的，空闲空间被分割成了两段，但很常见。

最后一个例子：现在假设剩余的两块已分配的空间也被释放。没有合并，空闲链表将非常破碎，如下图所示。

[![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures13.png)](https://pic.taifua.com/Picture/website/ostep/c17/13.png)

从图中可以看出，现在一团糟！为什么？简单，忘了合并链表项，虽然整个内存空间是空闲的，但却被分成了小段，因此形成了碎片化的内存空间。解决方案很简单：遍历链表，合并相邻块。完成之后，堆又成了一个整体。
### 空闲空间的分配策略
既然有了这些底层机制，现在来看看管理空闲空间的一些基本策略。

**理想的分配程序可以同时保证快速和碎片最小化**。遗憾的是，由于分配及释放的请求序列是任意的（毕竟，它们由用户程序决定），任何特定的策略在某组不匹配的输入下都会变得非常差。所以不会描述 “最好” 的策略，而是介绍一些基本的选择，并讨论它们的优缺点。

### 1. 最优匹配

最优匹配策略非常简单：**首先遍历整个空闲链表，找到和请求大小一样或更大的空闲块，然后返回这组候选者中最小的一块**。这就是所谓的最优匹配（也可以称为最小匹配）。只需要遍历一次空闲链表，就足以找到正确的块并返回。最优匹配背后的想法很简单：**选择最接近用户请求大小的块**，从而尽量避免空间浪费。然而，这有代价。简单的实现在遍历查找正确的空闲块时，要付出**较高的性能代价**。

### 2. 最差匹配

最差匹配方法与最优匹配相反，**它尝试找最大的空闲块，分割并满足用户需求后，将剩余的块（很大）加入空闲链表**。最差匹配尝试在空闲链表中保留较大的块，而不是向最优匹配那样可能剩下很多难以利用的小块。但是，最差匹配同样需要遍历整个空闲链表。更糟糕的是，大多数研究表明**它的表现非常差，导致过量的碎片，同时还有很高的开销**。

### 3. 首次匹配

首次匹配策略就是**找到第一个足够大的块，将请求的空间返回给用户**。同样，剩余的空闲空间留给后续请求。首次匹配有速度优势（不需要遍历所有空闲块），但有时会让空闲链表开头的部分有很多小块。因此，分配程序如何管理空闲链表的顺序就变得很重要。一种方式是**基于地址排序**。通过保持空闲块按内存地址有序，合并操作会很容易，从而减少了内存碎片。

### 4. 下次匹配

不同于首次匹配每次都从链表的开始查找，**下次匹配算法多维护一个指针，指向上一次查找结束的位置。其想法是将对空闲空间的查找操作扩散到整个链表中去，避免对链表开头频繁的分割**。这种策略的性能与首次匹配很接近，同样避免了遍历查找。
## 虚拟内存的分配方式
### Linux内存布局
在 Linux 操作系统中，虚拟地址空间的内部又被分为**内核空间和用户空间**两部分，不同位数的系统，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，如下所示：

![图片](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures1db038e1d2e5325b05e2bb80475d962a.png)



通过这里可以看出：

- `32` 位系统的内核空间占用 `1G`，位于最高处，剩下的 `3G` 是用户空间；
- `64` 位系统的内核空间和用户空间都是 `128T`，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。

虽然每个进程都各自有独立的虚拟内存，但是**每个虚拟内存中的内核地址，其实关联的都是相同的物理内存**。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。

![图片](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesc88bda5db60029f3ea57e4306e7da936.png)

#### 用户空间的虚拟内存布局
用户空间和内核空间划分的方式是不同的，用户空间内存从**低到高**分别是 6 种不同的内存段：

![图片](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures7b5b6b3728acde8df019350df3cb85c1.png)



- 程序文件段，包括二进制可执行代码；
- 已初始化数据段，包括静态常量；
- 未初始化数据段，包括未初始化的静态变量；
- 堆段，包括动态分配的内存，从低地址开始向上增长；
- 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（跟硬件和内核版本有关）；
- 栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 `8 MB`。当然系统也提供了参数，以便我们自定义大小；

在这 6 个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 `malloc()` 或者 `mmap()` ，就可以分别在堆和文件映射段动态分配内存。

##### Linux内核对用户态的虚拟内存空间（用户空间）的管理

![image.png](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesf28079c8b8fbf1853570302bee2a1929.png)



#### 内核空间的虚拟内存布局

![image.png](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures0d19dc439390c46612e31ee973f83145.png)

##### 1. 直接映射区

![image.png](https://cdn.xiaolincoding.com//mysql/other/1cf5fc48692826d8446638bbc5dd0e0b.png)
**内核态的前 896M 是直接映射区，用于存放内核代码、数据和程序相关的数据结构，比如页表、用户态的虚拟地址空间结构 mm_struct、vm_area_struct 等。**
###### ZONE_DMA
在X86体系结构下，ISA总线的DMA (直接内存存取)控制器，只能对内存的前16M进行寻址，这就导致了ISA设备不能在整个32位地址空间中执行DMA，只能使用物理内存的前16M进行DMA操作。
因此直接映射区的前16M专门让内核用来为DMA分配内存，这块16M大小的内存区域我们称之为ZONE_DMA。
###### ZONE_NORMAL
而直接映射区中剩下的部分也就是从16M到896M(不包含896M）这段区域，我们称之为
ZONE_NORMAL。从字面意义上我们可以了解到，这块区域包含的就是正常的页框(使用没有任何限制)。
ZONE_NORMAL由于也是属于直接映射区的一部分，对应的物理内存16M到896M这段区域也是被直接映射至内核态虚拟内存空间中的3G＋16M到3G +896M这段虚拟内存上。

##### 2. 8M空洞
内核虚拟内存空间中的3G ＋896M这块地址在内核中定义为high_memory，high_memory往上有一段8M大小的内存空洞。空洞范围为: high_memory 到VMALLOC_START。

##### 3. vmalloc动态映射区

![image.png](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures0bd4766b19d043bb4aebdd06bdf8e67c.png)

接下来VMALLOC_START到VMALLOC_END之间的这块区域成为动态映射区。采用动态映射的方式映射物理内存中的高端内存。

和用户态进程使用malloc申请内存一样，在这块动态映射区内核是使用vmalloc进行内存分配。由于之前介绍的动态映射的原因，vmalloc分配的内存在虚拟内存上是连续的，但是物理内存是不连续的。通过页表来建立物理内存与虚拟内存之间的映射关系，从而可以将不连续的物理内存映射到连续的虚拟内存上。

##### 4. 永久映射区
而在PKMAP_BASE到 FIXADDR_START之间的这段空间称为永久映射区。在内核的这段虚拟地址空间中允许建立与物理高端内存的长期映射关系。比如内核通过alloc_pages()函数在物理内存的高端内存中申请获取到的物理内存页，这些物理内存页可以通过调用kmap映射到永久映射区中。
##### 5. 固定映射区
内核虚拟内存空间中的下一个区域为固定映射区，区域范围为: FIXADDR_START到 FIXADDR_TOP。
在固定映射区中的虚拟内存地址可以自由映射到物理内存的高端地址上，但是与动态映射区以及永久映射区不同的是，在固定映射区中虚拟地址是固定的，而被映射的物理地址是可以改变的。也就是说，有些虚拟地址在编译的时候就固定下来了，是在内核启动过程中被确定的，而这些虚拟地址对应的物理地址不是固定的。采用固定虚拟地址的好处是它相当于一个指针常量（常量的值在编译时确定)，指向物理地址，如果虚拟地址不固定，则相当于一个指针变量。
那为什么会有固定映射这个概念呢?比如:在内核的启动过程中，有些模块需要使用虚拟内存并映射到指定的物理地址上，而且这些模块也没有办法等待完整的内存管理模块初始化之后再进行地址映射。因此，内核固定分配了一些虚拟地址，这些地址有固定的用途，使用该地址的模块在初始化的时候，将这些固定分配的虚拟地址映射到指定的物理地址上去。
##### 6. 临时映射区

![image.png](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesfcb8b59a4b73a823603b6cbd4f720b5d.png)

内核调用iov_iter_copy_from_user_atomic函数将用户空间缓冲区 DirectByteBuffer中的待写入数据拷贝到page cache 时，内核不能直接进行拷贝，因为此时从page cache 中取出的缓存页page 是物理地址，而在内核中是不能够直接操作物理地址的，只能操作虚拟地址。
那怎么办呢?所以就需要使用kmap_atomic将缓存页临时映射到内核空间的一段虚拟地址上，这段虚拟地址就位于内核虚拟内存空间中的临时映射区上，然后将用户空间缓存区DirectByteBuffer 中的待写入数据通过这段映射的虚拟地址拷贝到page cache 中的相应缓存页中。这时文件的写入操作就已经完成了。
由于是临时映射，所以在拷贝完成之后，调用kunmap_atomic将这段映射再解除掉。


### malloc

malloc() 并不是系统调用，而是 C 库里的函数，用于动态分配内存。**malloc() 分配的是虚拟内存**。如果分配后的虚拟内存没有被访问的话，虚拟内存是不会映射到物理内存的，这样就不会占用物理内存了。只有在访问已分配的虚拟地址空间的时候，操作系统通过查找页表，发现虚拟内存对应的页没有在物理内存中，就会触发缺页中断，然后操作系统会建立虚拟内存和物理内存之间的映射关系。

malloc 申请内存的时候，会有两种方式向操作系统申请堆内存。
#### brk()
通过 brk() 系统调用从堆分配内存,如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；
通过 brk() 函数将「堆顶」指针向高地址移动，获得新的内存空间。如下图：

![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesbrk%E7%94%B3%E8%AF%B7.png)

malloc 通过 **brk()** 方式申请的内存，free 释放内存的时候，**并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用**.
##### brk()优缺点
- 优点：
malloc 通过 brk() 系统调用在堆空间申请内存的时候，由于堆空间是连续的，所以直接预分配更大的内存来作为内存池，当内存释放的时候，就缓存在内存池中。等下次在申请内存的时候，就直接从内存池取出对应的内存块就行了，而且可能这个内存块的虚拟地址与物理地址的映射关系还存在，这样不仅减少了系统调用的次数，也减少了缺页中断的次数，这将大大降低 CPU 的消耗。
- 缺点：
随着系统频繁地 malloc 和 free，尤其对于小块内存，堆内将产生越来越多不可用的碎片，导致“内存泄露”。而这种“泄露”现象使用 valgrind 是无法检测出来的。
#### mmap()
通过 mmap() 系统调用在文件映射区域分配内存；如果用户分配的内存大于 128 KB，则通过 mmap()  申请内存；
通过 mmap() 系统调用中「私有匿名映射」的方式，在文件映射区分配一块内存，也就是从文件映射区“偷”了一块内存。如下图：

![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesmmap%E7%94%B3%E8%AF%B7.png)

malloc 通过 **mmap()** 方式申请的内存，free 释放内存的时候，**会把内存归还给操作系统，内存得到真正的释放**。
##### mmap()优缺点
- 优点：
分配大内存，不会产生内存泄露
- 缺点：
**频繁通过 mmap 分配的内存话，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大**

## 物理内存的分配方式
上面介绍的都是虚拟内存的分配方式，那么当虚拟内存被使用，发生缺页中断，操作系统是如何找到空闲的物理内存并将其与虚拟内存建立映射关系呢

### 物理内存的区域划分

![image.png](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Picturesc94df404b28ec5d4e1fc32e30fb0764b.png)

#### 1. ZONE_DMA
在X86体系结构下，ISA总线的DMA(直接内存存取）控制器，只能对内存的前16M进行寻址，这就导致了ISA设备不能在整个32位地址空间中执行DMA，只能使用物理内存的前16M进行DMA操作。
因此直接映射区的前16M专门让内核用来为DMA分配内存，这块16M大小的内存区域我们称之为ZONE_DMA。用于那些无法对全部物理内存进行寻址的硬件设备，进行DMA时的内存分配。例如前边介绍的ISA设备只能对物理内存的前16M进行寻址。该区域的长度依赖于具体的处理器类型。
#### 2.ZONE_DMA32
与ZONE_DMA区域类似，该区域内的物理页面可用于执行DMA 操作，不同之处在于该区域是提供给32位设备(只能寻址4G物理内存）执行DMA操作时使用的。该区域只在64位系统中起作用，因为只有在64位系统中才会专门为32位设备提供专门的DMA区域。
#### 3.ZONE_NORMAL
这个区域的物理页都可以直接映射到内核中的虚拟内存，由于是线性映射，内核可以直接进行访问。
#### 4.ZONE_HIGHMEM
这个区域包含的物理页就是我们说的高端内存，内核不能直接访问这些物理页，这些物理页需要动态映射进内核虚拟内存空间中(非线性映射)。该区域只在32位系统中才会存在，因为64位系统中的内核虚拟内存空间太大了(128T)，都可以进行直接映射。
#### 5. ZONE_DEVICE
为支持热插拔设备而分配的非易失性内存(Non Volatile Memory )，也可用于内核崩溃时保存相关的调试信息。
#### 6. ZONE_MOVABLE 
内核定义的一个虚拟内存区域，该区域中的物理页可以来自于上边介绍的几种真实的物理区域。该区域中的页全部都是可以迁移的，主要是为了防止内存碎片和支持内存的热插拔。


### 伙伴系统

因为合并对分配程序很关键，所以人们设计了一些方法，让合并变得简单，一个好例子就是**二分伙伴分配程序（binary buddy allocator**）。

在这种系统中，**空闲空间首先从概念上被看成大小为 2^N 的大空间**。当有一个内存分配请求时，空闲空间被递归地一分为二，直到刚好可以满足请求的大小（再一分为二就无法满足）。这时，请求的块被返回给用户。在下面的例子中，一个 64KB 大小的空闲空间被切分，以便提供 7KB 的块：

[![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures17.png)](https://pic.taifua.com/Picture/website/ostep/c17/17.png)

在这个例子中，最左边的 8KB 块被分配给用户（如上图中深灰色部分所示）。请注意，这种分配策略只允许分配 2 的整数次幂大小的空闲块，因此会有**内部碎片**的麻烦。

**伙伴系统运转良好的原因，在于很容易确定某个块的伙伴**。怎么找？仔细想想上面例子中的各个块的地址。如果你想得够仔细，就会发现**每对互为伙伴的块只有一位不同**，正是这一位决定了它们在整个伙伴树中的层次。
### Slab机制（分离空闲链表）
分离空闲链表的基本想法很简单：**如果某个应用程序经常申请一种（或几种）大小的内存空间，那就用一个独立的链表，只管理这样大小的对象。其他大小的请求都交给更通用的内存分配程序**。

这种方法的好处显而易见。**通过拿出一部分内存专门满足某种大小的请求，碎片就不再是问题了**。而且，**由于没有复杂的链表查找过程，这种特定大小的内存分配和释放都很快**。

就像所有好主意一样，这种方式也为系统引入了新的**复杂性**。例如，应该拿出多少内存来专门为某种大小的请求服务，而将剩余的用来满足一般请求？超级工程师 Jeff Bonwick 为 Solaris 系统内核设计的厚块分配程序（slab allocator），很优雅地处理了这个问题。

具体来说，在内核启动时，它为可能频繁请求的内核对象创建一些对象缓存，如锁和文件系统 inode 等。这些的对象缓存每个分离了特定大小的空闲链表，因此能够很快地响应内存请求和释放。**如果某个缓存中的空闲空间快耗尽时，它就向通用内存分配程序申请一些内存厚块（slab）（总量是页大小和对象大小的公倍数）**。相反，如果给定厚块中对象的引用计数变为 0，通用的内存分配程序可以从专门的分配程序中回收这些空间，这通常发生在虚拟内存系统需要更多的空间的时候。

厚块分配程序比大多数分离空闲链表做得更多，它将链表中的空闲对象保持在预初始化的状态。Bonwick 指出，**数据结构的初始化和销毁的开销很大。通过将空闲对象保持在初始化状态，厚块分配程序避免了频繁的初始化和销毁，从而显著降低了开销**。

# 缺页中断与内存页面转换算法
## 缺页中断
当 CPU 访问的页面不在物理内存时，便会产生一个缺页中断，请求操作系统将所缺页调入到物理内存。那它与一般中断的主要区别在于：

- 缺页中断在指令执行「期间」产生和处理中断信号，而一般中断在一条指令执行「完成」后检查和处理中断信号。
- 缺页中断返回到该指令的开始重新执行「该指令」，而一般中断返回回到该指令的「下一个指令」执行。

我们来看一下缺页中断的处理流程，如下图：

![缺页中断的处理流程](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures/%E7%BC%BA%E9%A1%B5%E5%BC%82%E5%B8%B8%E6%B5%81%E7%A8%8B.png)

1. 在 CPU 里访问一条 Load M 指令，然后 CPU 会去找 M 所对应的页表项。
2. 如果该页表项的状态位是「有效的」，那 CPU 就可以直接去访问物理内存了，如果状态位是「无效的」，则 CPU 则会发送缺页中断请求。
3. 操作系统收到了缺页中断，则会执行缺页中断处理函数，先会查找该页面在磁盘中的页面的位置。
4. 找到磁盘中对应的页面后，需要把该页面换入到物理内存中，但是在换入前，需要在物理内存中找空闲页，如果找到空闲页，就把页面换入到物理内存中。
5. 页面从磁盘换入到物理内存完成后，则把页表项中的状态位修改为「有效的」。
6. 最后，CPU 重新执行导致缺页异常的指令。

上面所说的过程，第 4 步是能在物理内存找到空闲页的情况，那如果找不到呢？

找不到空闲页的话，就说明此时内存已满了，这时候，就需要「页面置换算法」选择一个物理页，如果该物理页有被修改过（脏页），则把它换出到磁盘，然后把该被置换出去的页表项的状态改成「无效的」，最后把正在访问的页面装入到这个物理页中。

这里提一下，页表项通常有如下图的字段：

![](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures/%E9%A1%B5%E8%A1%A8%E9%A1%B9%E5%AD%97%E6%AE%B5.png)

那其中：

- *状态位*：用于表示该页是否有效，也就是说是否在物理内存中，供程序访问时参考。
- *访问字段*：用于记录该页在一段时间被访问的次数，供页面置换算法选择出页面时参考。
- *修改位*：表示该页在调入内存后是否有被修改过，由于内存中的每一页都在磁盘上保留一份副本，因此，如果没有修改，在置换该页时就不需要将该页写回到磁盘上，以减少系统的开销；如果已经被修改，则将该页重写到磁盘上，以保证磁盘中所保留的始终是最新的副本。
- *硬盘地址*：用于指出该页在硬盘上的地址，通常是物理块号，供调入该页时使用。


这里我整理了虚拟内存的管理整个流程，你可以从下面这张图看到：

![虚拟内存的流程](https://raw.githubusercontent.com/qhbsss/Pictures/main/Blog_Pictures/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B.png)
## 页面转换算法
页面置换算法的功能是，**当出现缺页异常，需调入新页面而内存已满时，选择被置换的物理页面**，也就是说选择一个物理页面换出到磁盘，然后把需要访问的页面换入到物理页。

那其算法目标则是，尽可能减少页面的换入换出的次数，常见的页面置换算法有如下几种：

- 最佳页面置换算法（*OPT*）
- 先进先出置换算法（*FIFO*）
- 最近最久未使用的置换算法（*LRU*）
- 时钟页面置换算法（*Lock*）
- 最不常用置换算法（*LFU*）